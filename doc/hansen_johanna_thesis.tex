%% LyX 2.0.5.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english]{report}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{longtable}
\usepackage{float}
\usepackage{calc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}

%graphicx is not part of included utsa thesis
\usepackage{graphicx}
%\usepackage{subcaption}
\graphicspath{{./figures/}}
\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}[chapter]
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage[ms]{UTSAthesis}      
\usepackage{times}            
\usepackage{latexsym}

\newenvironment{ruledcenter}{%
  \begin{center}
  \rule{\textwidth}{1mm} } {%
  \rule{\textwidth}{1mm} 
  \end{center}}%


  \theoremstyle{definition}
  \newtheorem{defn}{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

  \providecommand{\definitionname}{Definition}
\providecommand{\theoremname}{Theorem}

\begin{document}
\bibliographystyle{acm}
%\committee{Sos Agaian, Ph.D., Chair}{Prof Hanumant Singh, Ph.D.}{Prof. C, Ph.D.}{Prof. D, Ph.D.}{Prof. E, Ph.D. }
\supervisor{Sos Agaian, Ph.D.}
\cosupervisor{Hanumant Singh}
\committeeC{Prof A}
\committeeD{Prof A}
\committeeE{Prof A}

\informationitems{Master of Science in Electrical Engineering}{M.Sc.}{Department of Electrical Engineering}{College of Engineering}{December}{ 2016 }


\thesiscopyright{Copyright 2016 Johanna Hansen \\
All rights reserved. }


\dedication{\emph{I would like to dedicate this thesis to the many mentors and 
teachers who have guided me throughout this process.}}


\title{\textbf{Monitoring Sea Ice with Unmanned Aerial Vehicles}}


\author{Johanna Hansen}
\maketitle
\begin{acknowledgements}

Thanks!

\end{acknowledgements}

\begin{abstract}

    Unmanned Aerial Vehicles (UAVs) are providing revolutionary  
real-time access to the terrain around us. The low cost and accessability of this technology has empowered military, agricultural, commercial, and recreational applications with new sensing capabilities. 
This thesis will focus on the application of low-altitude aerial photography for sea ice monitoring, useful for polar sea expidentions and research endeavors as scientists 
try to better understand the world's changing climate. 
For ships navigating in ice laden waters, UAVs provide the potential
to replace manned helicopters acting as scouts for 
 ships navigating in ice laden waters. 
 As one can imagine, manned helicopters 
 are expensive to purchase and operate and require an experienced pilot. They also consume a large footprint of valuable deck space on an ocean-going vessel. 

Though UAVs provide an excellent data collection platform, 
that data can be labor intensive to process and interpret. Images captured from 
a UAV provide a small field-of-view. This means that for a typical scouting mission, hundreds, if not thousands of 
images are collected. The common way to organize 
this information is by joining the overlapping images into a single field of
view by relating the images into a single image known as a mosaic. The process of creating a mosaic automatically requires 
finding overlapping scenes within neighboring images and then registering each image onto the mosaic. 
Although this is a fairly routine process with modern computers, traditional 
image mosaicing algorithms tend to fail when faced with sea ice 
scenes. Sea ice is difficult to mosaic because solid ice itself 
typically has very few distinguishing features from which 
to match to overlapping images. We must take into account the fact that water is 
not static over time so will not provide structural information about the scene
when images are captured at different points in time as is the case when a UAV 
with a monocular camera surveys a region. We provide a solution to the dynamic 
state of the images by first masking water and then matching only ice from the images. In addition to improving image registration, this step also allows 
us to automatically calculate the concentration of ice in the surveyed area. 
We take this a step further and provide a scheme to classify the types of ice 
encountered in the scene. This is important for large-scale route planning as 
different ice has various structural properties  which are important for 
ice breaking ships. This also creates a baseline for scientific surveys of a 
region. Our results are demonstrated using 
data obtained over several days on an Artic Expedition. 

\end{abstract}
\pageone{}

\chapter{Introduction}
Sea ice has a large impact on the global climate, maintaining a large heat sink 
in the poles and increasing the Earth's albedo \cite{Sephton94}. It is important to 
study because it can hinder travel and provide significant scientificant information. We propose to utilize low-cost unmanned aerial vehicles to 
make flexible, high-resolution surveys of sea ice. To make data collected useful, 
we provide an algorithm for mosaicing the sea ice into a single scene and 
classification tools to quantify ice types and concentration. 

Capturing images from a low-altitute provides detailed information of the 
target scene, but the tradeoff for resolution is field-of-view. To obtain understanding 
of a scene spread across many high resolution images with a small field-of-view, 
it necessary to combine the information into a larger image by mosaicing the images 
together into a single image. 

Image mosaicing has been utilized to join images of overlapping aerial
scenes since long before digital photographs \cite{Wolf83}. Originally, film 
photographs were captured from hot air balloons or low-flying aircraft and pasted 
together by hand. Later, satellites and digital photography advanced the field 
of remote sensing and we began to stitch and analyze large volumes of images 
with computers. Now, image stitching is commonplace and the technology is embedded 
in many smart phones equipped with cameras for use for stabilization, small panoramas, 
and multiple image super-resolution. Many dedicated software programs are 
available to provide image stitching including 
AutoPano, Hugin, PhotoScan \cite{AutoPano, Hugin, Photoscan}. In this work, we 
tackle a mosaicing for a dataset which remains challenging for conventional mosaicing pipelines 
and add a new capability that automatically classifies sea ice 
type at the pixel level providing ice coverage information. 

\section{Motivation}
Expeditions into the world's frozen oceans require expert navigation and 
real-time knowledge of the constantly shifting conditions. This region of the earth 
is interesting from a scientific perspective  
because much of it remains unexplored and because it is rapidly changing.
It also harbours rich natural resources 
that are increasingly accessible and important to government and commercial 
interests. However, to work and travel safely in these regions requires 
constant vigillance and near continous monitoring of the ever moving ice 
that can trap ships in these inhospitable environments. 
Real-time knowledge about the changing and often dangerous 
conditions is necessary for a ship working in polar regions. 
This surveillance 
still presents significant monetary expenditures and a technological challenge.

Historically, ships working in these conditions
have relied on human scouts, local radar, helicopters, and/or satellite imagery to monitor 
the sea ice movement.  However, these approaches are resource intensive in the 
case of helicopters, limited in view in the case of radar, and suboptimal in the case of satellites. 

Manned aircraft are expensive to operate, requiring 
expert pilots and precious deck space. They also do not necessarily solve the scientific 
problem of cataloguing the sea state. 
Severe weather or tumultuous seas can limit when a manned aircraft can be safely deployed. Often these times are when it is most important retrieve local conditions. 
Satellite sensors can provide useful
overviews, but the data captured is low-resolution both in time and coverage. It 
is also prone to occlusion by cloud cover or atmospheric conditions (again, these 
reports are often especially important when the weather is foul). 

Low-cost UAVs can fill the gap between wide-area satellite coverage and instanteanous reconissance. They can be thought of as nearly disposable in 
bad weather and the data may be retrieved remotely, even if the vehicle 
is unable to return to the ship. 
The  main
advantage over satelite imagery is obvious in that the 
data is captured at a much higher resolution and can be gathered at the operaters discretion. This means that data for a particular region can be updated whenever 
the vehicle is ready.

UAVs require little expertise to operate and little deck space. However, because 
there is no human observer of the terrain, the data collected from the vehicle must 
be processed and analyzed in a way that is useful for non-technical decision makers. 
Although UAVs can provide images with high resolution, each image only covers a 
small part of the interesting area.
 This means that a single flight can result in thousands 
of images that must be organized into a human-readable format. 
A process called image mosaicing must be performed to merge overlapping images 
of the terrain into a single high resolution image that is easy to comprehend and 
integrate into existing workflows. 
Image registration is the process of developing a pixel-to-pixel mapping between 
the views in the distinct images. 
By registering the many images into a single image, 
we create a composite scene called a mosaic with a large field of 
view, while maintaining high resolution.  

Several commercially available programs 
have been applied to sea ice dataset with disappointing results. 
However, our experminents indicate that these packages were not 
reliable for developing mosaics with sea ice 
data. The large volume of images with poor features that results from a typical  UAV sea 
ice survey seemed to incapacitate the algorithms available in commercial software.
When the surveys were divided 
into geographic batches, 
stitching and warping errors typically resulted from the accuracy of feature 
matching or the projection of object height. 


Sea ice images present a 
fairly unique problem in that the images contain largely repeating, homogenous 
pixels that do not perform well with traditional techniques.
We add an additional preliminary step to the typical stitching pipeline to detect 
known nonstationary features and remove them from the pixels that are considered for 
feature detection. We use these features to gain understanding about ice floes and 
provide an estimate of sea ice coverage based on this measurement. 


\section{Problem Statement}
The problem of establishing correspondences between a set of images taken from 
different viewpoints is well studied in computer vision. Applications of this 
type of work include motion recovery, image retreival, and robot navigation 
to name a few. We limit the correspondence problem to image mosaicing and tackle the difficult issue of determining overlap 
 for images which are nearly featureless such as solid ice sheets. 
In addition we tackle the problem of determining sea ice concentration and ice types, an important problem when planning routes by sea for surface and underwater vessels. We see our terrain classification solution as an especially valuable first step in collecting data needed  for  extending 
autonomous mission planning capabilities in multi-vehicle collaborations. 




Considering the aforementioned goals 
and challenges, we address and solve the three following problems: 
\begin{itemize}
    \item{Segment sea ice from water to provide ice concentration and reduce false matches in feature matching (performed in subsequent steps)}
    \item{Classify ice regions by type of ice seen in floes}
    \item{Exploit metadata from the UAV to speed up image matching and 
            registration by reducing the search space of overlapping images and by 
            pre-aligning images so that low-cost feature descriptors can be used for 
            matching.}
    \item{Aligning images when metadata is not available by escalating feature 
            extraction complexity in regions with poor features.
    This iterative scheme adds additional complex only as needed, using fast descriptors when possible and slower, more robust methods only when needed. 
    }
\end{itemize}

\section{Outline}

The remainder of this thesis is organized as follows. Chapter \ref{chap:background} 
provides a history of image mosaicing and stitching approaches. 
Chapter \ref{chap:related} discusses related work in sea ice and image mosaicing 
tasks of aerial and underwater imagery. In Chapter \ref{chap:features} we discuss 
the process of assessing feature descriptor performance on this dataset and present 
performance results of common approaches. Segmenting sea ice from the surrounding 
water and size estimation considered in Chapter \ref{chap:segmentation}. Image matching, 
registration, and blending is presented in Chapter \ref{chap:registration}. In Chapter 
\ref{chap:results} sample results are provided with a discussion and evaluation 
of the approach. Finally, in Chapter \ref{chap:conclusion}, a summary and goals 
for future work is presented. 

We assume that the readers have basic knowledge of digital photography and 
aerial vehicles, however basic concepts are summarized here before 
delving into more complicated computational concepts. 

\chapter{Background}
\label{chap:background}
\section{Remote Sensing}
Remote sensing is the term used to describe the process of obtaining data without 
physical contact. The field of remote sensing, like most modern fields that 
are associated with data, has experienced an explosion in the amount, quality, and 
accessibility of data. 
 The images that we are working with in this 
thesis are from the visible spectrum and try to represent the world as they are seen 
with the human eye, however, often remote sensing is performed with other types of sensors. Passive sensors, like cameras, gather electromagnetic radiation that 
it has not emitted itself. Passive sensors are also often used in other electrmagnetic 
frequency bands like infrared or thermal bands for remote sensing.  Remote sensing data
can also be gathered using active sensors such as RADAR (Radio Detection And Ranging) or LiDAR (LIight Detection and Ranging) . In the case of active sensors, 
the sensor emits energy at a particlar band and measures the reflectance returned. Remote sensors also take form in the lower, acoustic end of the spectrum in the case of SONAR (SOund Navigation And Ranging) which measures 
reflectance in the acoustic spectrum (usually underwater). 

Traditionally, remote sensing typically referred to spatial data captured from platforms such as satellites or manned aircraft.  
These platforms generally 
observe the earth from a higher altitude and thus a lower 
resolution than what we are concerned with here and 
thus have looser requirements for stitching accuracy. For example, a 
popular remote sensing instrument provided by NASA 
is MODIS \cite{MODIS}, the Moderate Resolution Imaging 
Spectroradiometer. A MODIS is housed on two satellites that together 
capture most of the 
earths surface at 36 different wavelengths every 1 to 2 days. Though this is 
impressive coverage, the imagery is only at a resolution of about 
 500 m in the visible spectrum. 

Data collected from small 
unmanned aircraft is much more flexible in terms of capture frequency and 
location. However, potential coverage area is also much more limited with a 
single sensor when compared to high-altitude systems.  Although our system 
seeks to extend remote sensing capabilities of sea ice to UAVs, we will further 
investigate past work in this field performed with high altitude systems  in 
Section \ref{sec:highalt}. In the following section, we cover UAVs in more detail 
to lay the groundwork for the remainder of the paper. 

With the abundance of remote sensing data has come an increasing need for 
automated methods of interpreting the data.  In Sections \ref{sec:ice} and 
\ref{sec:map} we provide background on two data products that enable faster 
 assesment and interpretation of the terrain captured by digital cameras and 
 that we improve upon in this thesis.

\section{Unmanned Aerial Vehicles as a Platform for Visual Remote Sensing}
Remote sensing has long been dominated by 
governments and commercial organizations, but recent 
advances in unmanned aerial vehicles have brought flight to 
the average consumer and researcher. 
Individual UAVs vary widely in terms of payload, endurance, weight, 
and speed and a comprehensive review is out of the scope of this thesis.
For a more complete overview of UAV capilities and history, 
please refer to a review by Colomina \cite{Colomina14}. For now, we'll give a 
brief introduction to a minimal set of sensors needed to perform our experiments.  

One important consideration for shipboard 
operations is the space needed for launch and recovery. There are two general 
types of architectures of UAVs: fixed wing and rotary winged. Fixed wing vehicles,
which are shaped like a airplane, are 
typically more stable and have longer range than rotary wing vehicles, which
propel themselves like a helicopter. However, fixed-wing vehicles have 
a more complicated deployment when compared to rotary-winged vehicles, 
requiring horizontal space for takeoff and landing. 
This can be difficult on a ship with limited deck space. Rotary-winged vehicles, 
like helicopters are generally more agile than fixed-wing aircraft which can 
be advantageous in many situations. 

UAVs vary greatly 
in their computational capabilities and complexity. Many are capable of 
autonomous flight, while others rely solely on remote control by humans. For 
the purpose of this research, 
we disregard flight planning and control and focus only on the data products collected 
from the platform. 

\subsection{UAV Sensors}
Nearly all systems will have some sensing capabilities necessary for navigation, 
though the accuracy and capabilities of these devices can vary widely and 
have enormous impact on the quality of flight and data products captured.

\subsubsection{Global Positioning System (GPS)}
The Global Positioning System, frequently shortened to GPS, is a system which 
provides the location and time when a receiver is within view 
of at least four system's satellites. A GPS receiver monitors the 
the time of flight for messages passed between each of the visible satellites and the receiver and uses this to perform a calculation of the receiver position. 
This position is presented as latitude, longitude, 
and elevation from mean sea level. In our system, GPS points are the main method 
of UAV localization. Although the GPS provides frequent updates for the vehicle position, this calculation contains some ammount of error that is almost always too large to perform image stitching at any reasonable resolution. GPS accuracy 
can vary depending on atmospheric effects and receiver quality, but generally 
location is reported correctly to within a few meters. 


\subsubsection{Inertial Measurement Unit (IMU)}
An inertial measurement unit (IMU) is a sensor that utilizes accelerometers, 
gyroscopes, and magnetometers situated orthogonally to each other to estimate the pitch, roll, and heading. 
This device provides important information  
 for both navigation (especially in the absense of GPS) and 
for determining how images captured from the UAV are oriented on the earth. 
Though IMUs are useful for rough estimates of pose, they tend to suffer from 
accumulated error that makes them undesireable for use for long term navigation.



\subsubsection{Digital Cameras}
Digital cameras have become ubiquitous and convienent sensors for
capturing and recording the space around us. They are optical instruments that record light using an image sensor and record it to memory. 

There are several parameters that are important when considering cameras as a
sensor. The lens of the camera
serves the role of focusing light onto the light sensor. 
The focal length of the lens is the distance between the lens and 
the image sensor. Image resolution is a measure of how much detail an image holds. A higher resolution means that an image 
sensor is able to better observe the smallest object for a given lens. We typically refer to resolution by referring to the number of pixels in RowsxColumns that are presented in am image where a pixel is the smallest 
addressable element in the image. 
Although an image mosaic can be computed without any knowledge of 
the internal camera calibration parameters such as focal length, 
optical center, or relative camera motion between frames, these parameters 
are often necessary when relating imagery to the physical features such as 
when relying on vision for navigation and obstacle avoidance. 

TODO: calibration?

Moving platforms which utilize cameras for mapping are typically configured 
as narrow-baseline or wide-baseline. 
Narrow-baseline stereo vision, or simply 
stereo vision, involves mounting two cameras at a known distance from each other 
and capturing images simultaneously from both sensors.  This is advantageous in 
many situations because it allows us to calculate the offset between the same 
object captured in the two images
when compared to the known baseline between the cameras.  This setup can 
be used for 3D mapping and distance calculation of objects captured by both 
cameras \cite{Olson10}.
Wide-baseline, or monocular systems capture the scene from different positions 
using the same camera. Unlike stereo vision, monocular vision systems 
do not have an easy method of determining the distance to objects in photos 
from a single location and must rely on movement of the sensor to characterize objects from diffferent viewpoints. Though narrow-baseline systems are attractive 
for mapping 3D scenes, they are fundamentally limited in their ability to measure 
depth by the distance of the baseline. 
The accuracy of stereo degrades as the distance to the scene increases and is 
lower with a larger baseline distance. 
In addition, a large baseline is not feasible on many
platforms, especially airborne systems. Rather than using two fixed cameras 
capturing the scene simultaneously, we use a wide-baseline, monocular system.
This distance estimation is inaccurate when compared to the strictly known 
viewpoint disparity in traditional stereo vision systems and determining the o
overlapping scene with wide-baseline can be more difficult than traditional stereo 
systems, because there is often a larger difference in perspective.
Since the influential paper by Schmid and Mohr \cite{Schmid97} on wide-baseline stereo, many new algorithms have been introduced for making sense of this type of data. We will discuss this more in the next section. 




\section{Vision Based Mapping}
The problem of determining correspondence between two images is a well 
studied problem in computer vision.  

An image mosaic is a synthetic wide-angle camera view which is built from a set of smaller images with an overlapping field of view. 
Image mosaicing has been extensively used to gain a greater viewpoint of the world and there exists a large and robust body of work for creating mosaics from digital photographs beginning in 1975 with Milgram's use of phase correlation for image alignment  \cite{Milgram75} and marked with seminal papers introduced by Brown in 2003 for recognizing panoramas \cite{Brown03} as well as many more  
in mosaicing \cite{Szeliski95, Schmid97, Shum00, Brown03 }.
Historically, images were captured on air balloons and airplanes and then printed 
and aligned by hand. In modern times however, digital cameras and smartphones 
are capable of automatically stitching small panoramas with limited 
computational capability. 
Panoramas are a less general form image mosaicing in which camera motion is 
limited to rotation. It serves as a platform where much research is perfomed, but 
is too simplistic of a model for our work. 
Images captured from low-altitude UAV are challenging when compared to 
panoramic construction
or low-resolution satellite image mosaicing. 



Most of the work and current research is found in the first step, as determining 
overlapping scenes from different viewpoints or in the face of drastic changes 
is still quite difficult.  Common changes in scene include 
differences in illumination, a rotated view, scaling, or moving objects. In the 
following sections, we'll give a step-by-step introduction to the process of 
image mosaicing with historical context and then we'll provide an overview of 
the current state of research related to the problems faced in our mosaicing problem.
For a more thourough review of the  history and techniques for building image mosaics see work by Szeliski in \cite{Microsoft}, Prathap in \cite{Prathap16} and  Zitova in \cite{Zitova03}.

Constructing an image mosaic typically consists of several general steps:

\begin{enumerate}
    \item{\textbf{Determine images with overlapping views of a scene.} }
    \item{\textbf{Estimate the homography between images with overlapping scenes.}}
    \item{\textbf{Register images onto a common frame.}}
    \item{\textbf{Blend image seams to improve appearance.}}
\end{enumerate}
 
Many researchers and engineers have utilized UAVs to gather images to form large-scale 
photo mosaics \cite{ Caballero07, Yahyanejad10, Cheng10, Pritt14, Kekec14, Prasad16, Vousdoukas11, Wang11, Wang14}.
Images collected with a UAV can be quickly, but roughly  aligned 
using data from sensors such as Global Positioning System (GPS) points and 
Intertial Measurement Unit (IMU) estimates.
However, these measurements are typically not 
accurate or reliable enough to generate a visually appealling image. 
Instead, most approaches for mosaicing UAV images use some form of a feature 
based approach to match images which we will discuss in the next few sections.

Problems tend to occur when there exists 
low-overlap between images, moving objects in the survey area, or featureless 
images. 


\subsection{Detecting Image Overlap}
Relating the images 
requires overcoming noise and finding unrelated, unknown translations, scaling, 
and rotations between images. 
Usually, a reasonable amount of scene overlap (about 15-30\% overlap) with distinctive features 
is typically required to successfully deterimine if two images should be stitched together. 

Methods of determining image similarity generally falls into two camps:
direct and feature based approaches. Direct methods perform correlations or 
convolution operations over the prospective
images to measure their similarity. This involes 
a computation across all of the pixels in the image and tends to be
slow and memory intensive 
\cite{Lucas81, Barnea72, Szeliski95, Szeliski97, Shum-local98, Irani00}. 
However, direct methods are able to solve for motion of the 
camera and the correspondence of every pixel at the same time. Because this approach 
compares pixel values directly, it is prone to falter under illumination changes. 
The more modern approach is feature based..
Feature
based methods only measure the similarity of images based on 
interesting regions of the image, such as lines, blobs, or edges. 
The interesting regions are called \emph{features} 
or \emph{keypoints}. Keypoints within the image must be found algorithmically 
by a keypoint detection algorithm. Then the keypoint is described by a 
\emph{descriptor} using a descriptor algorithm.  Descriptors have been developed so 
that they are invariant to many of the transformations that plague images. 
By comparing images using only their interesting features, the motion of the 
camera can be solved for using only the
parts of the image in which correspondence is easy to detect. This reduces 
computational waste and improves accuracy.
The geometry calculated using the descriptors 
is then used to transform all of the pixels in 
the image. 

The feature based technique was first described by Schmid, who utilized Gaussian derivitives 
to describe a keypoint in a manner that was rotationally invariant \cite{Schmid97}.
Feature based methods are typically the modern approach because they are 
generally less computationally intensive than direct methods and because the 
feature descriptors can be developed to be invariant to image
tranformations \cite{Schmid98, Lowe99, Brown03, Hu06, Elibol08, Rublee-ORB11, Alahi-FREAK12}. 
Like most modern methods, we utilize a feature based approach to 
determine image overlap. 
From this point onward in this paper, 
we will assume that overlap detection 
is performed using the feature based approach. 
Reliable  extraction of a manageable number of potentially 
corresponding features is a crucial prerequisite to successful mosaicing. The 
feature dectector and descriptor choice is of significant importance. 

One of the most well known and often used keypoint descriptors is Scale
Invariant Feature Transforms (SIFT) \cite{Lowe04}, which was introduced by Lowe in 2004. SIFT descriptors detects keypoints 
based on Difference of Gaussians (DOG). 
This algorithm
has consistently been the top performer in terms of  
scale and rotation invariance, though it is has considerable overhead. 
Since the introduction of SIFT, many SIFT-like descriptors have been 
introduced to 
reduce computation time.  
Speeded-Up Robust Features (SURF) \cite{Bay-SURF08}, for instance,
yields similar feature performance to SURF, but with 
faster computation time. The performance improvement is 
acomplished by describing keypoints with the responses of Haar-like filters. 
Haar features are global-texture based. Both SIFT and SURF use an orientation 
operator.  Both SURF and SIFT use vector-based features, usually with a length of $128$ values that can be slow to compare. 
%Good foundation paper on multiscale approach \cite{Brown05} ??
%- this seemed to have a good descript of sift: http://www.cs.cmu.edu/~rahuls/pub/cvpr2004-keypoint-rahuls.pdf

%ORB
Although SURF achieves some speed gain over SIFT, it is still not realistic 
for real-time applications on platorms with limited capability \cite{Bekele13}. 
Binary keypoint descriptors aim to provide lightweight and fast computation with 
good performance by building the descriptor such that each bit is independent.
A binary descriptor uses significantly less memory (512 bits) for a given keypoint when compared with a gradient descriptor (usually 64 or 128 floating points) \cite{Prathap16}. 
This allows Hamming distance to be used rather than Eucliden distance to measure 
similarity and results in significant speed in matching with only marginal performance penalties \cite{Bekele13, Heinly12}. 
ORB \cite{Rublee-ORB11} or Oriented FAST and Rotated BRIEF descriptor is one of these computationally 
efficient binary descriptors that can be used in real-time for many applications. 
Like its name suggests, it works off of the very efficient FAST keypoint 
detector \cite{Rosten-FAST06} and BRIEF descriptor \cite{Calonder-BRIEF10}.
ORB overcomes the lack or rotational invariance in the BRIEF descriptor by 
de-correlating BRIEF features under rotational invariance, however, ORB is
only somewhat scale invariant.

The FREAK \cite{Alahi-FREAK12} descriptor , introduced in 2012 by Alahi improves upon the BRISK
descriptor \cite{Leutenegger-BRISK11} and was 
inspired by the retinal pattern in the biological eye. 
FREAKs are in general faster to compute with lower memory load than 
 SIFT, SURF or BRISK. 


We also consider the Zernike descriptor which has been used successfully for 
matching images in low-contrast scenes \cite{Pizarro03}.  Zernike moment 
descriptors are a set of orthogonal polynomials that 
work well in the presence of 
 noise,  and large scale changes in a computationally efficient manner. 
 Only the magnitude of Zernike moments is rotationally invarient \cite{Pizarro04}.
We utilize the classic Harris keypoints \cite{Harris} for fast corner finding. 
Harris keypoints are detected at the local minima of an autocorrelation 
function that is invariant to geometric image transformation \cite{Schmid98}. 
\cite{Chen10}
\cite{Hwang08}


TODO: read and cite these
Used zernike to create mosaic - read more 
\cite{Zhanlong13}
\cite{dudek}
\cite{Badra98, Badra99}
\cite{Bin02}
Use this for equations
\cite{Ameyah07} 
\cite{Hwang08}
\cite{Chen10}

Learn descriptors using conv net. 
\cite{Simo15}
\cite{Doersch15}
\cite{MountainView}

Alahi et al. show that using a grid of well performing descriptors achieves better 
matching than using a single one to match an image region \cite{Alahi10}. 



In Chapter \ref{chap:feature} we evaluate different feature descriptors against 
our dataset.

\subsubsection{Matching Features}
After keypoints are extracted from two images, they must be compared for 
similarity to determine if the images contain regions of the same scene. 
The correctness of the final mosaic is highly dependent on the 
accuracy of each of the calculated 
geometric relationship between overlapping images. 


Binary-valued feature descriptors (such as BRISK, ORB, or BRIEF) allow efficient 
matching when compared to 
vector-based features. Binary features can be quickly compared using the 
Hamming distance between descriptors. For Vector-based features such as SIFT 
and SURF, approximate nearest-neighbor search is used to match features. 
\begin{equation}
    \label{eq:euclidean}
    d(m, m') = \sqrt{(m_1-m'_1)^2 +  (m_2-m'_2)^2 + ... + (m_n-m'_n)^2}
\end{equation}


The Euclidean distance or $L^2$ Norm (Equation \ref{eq:euclidean} is used to calculate the distance between potential matches for vector-based descriptors like SURF and SIFT of length $n$. 
Hamming distance (Equation \ref{eq:hamming} is used for 
determining the distance between pairs of  binary descriptors like ORB and FREAK of length $n$. 
\begin{equation}
    \label{eq:hamming}
    d(m, m') = \sum_{k=0}^{k=n}{|m_k - m'_k|}
\end{equation}
Non brute force methods can also be used for approximate matching such as the 
Flann matcher and Approximate nearest neighbors. 
Maximum Likelihood
Least Median of Squares (LMS),
---

 
Incorrect matches, which are especially common among images with 
repetitive features, should be filtered out if possible. 
Lowe showed that if the two best matches were close in distance, then the 
probability of a false match was high \cite{Lowe04}. To combat this, he 
proposed a simple \emph{Ratio Test}
to measure the distance of the best match over the distance 
of the second best match and eliminated the match if it was greater than a theshold. 


\subsection{Defining the Transformation}
After the corresponding pixels 
between overlapping images have been identified, the next step in the process 
of developing a mosaic is to estimate
the relationship between the images.
The process of finding this relationship is called \emph{homography}.   Homography is defined in Equation \ref{eq:homo}, where the  transformation matrix, $H$, maps points on one image, $i$, to corresponding points on the reference image or mosaic, $i'$. 
The transformations 
can be translation, similarity, affine, or projective which have 2, 4, 6, and 8 parameters respectively. 


\begin{equation}
x'=H*x
\label{eq:homo}
\end{equation}

\begin{equation}
x'=Ax+t
\label{eq:affine}
\end{equation}


\begin{equation}
    \label{eq:affineA}
       x' =
        \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
    \end{bmatrix}
        x 
\end{equation}

We will discuss each 
of these transformations in the following paragraphs as they are defined in 
\cite{SzeliskiBook}.  The most complex transformation,
a projective transform, only preserves straight lines and is the farthest an 
image can be transformed. This type of transformation is used in many mosaicing approaches, but can result in errors in instances with low overlap between 
images or when there are few distinctive features available \cite{Wang11}. We instead consider only affine tranformations in which parallel lines remain parrallel under transformation. 
An affine tranformation (defined in Equations \ref{eq:affine} and \ref{eq:affineA} describes scale, rotation, translation, and skew 
between two 2D planes in 2D space. In Equation \ref{eq:affineA}, A is written as an arbitrary $2x3$ matrix.  This model is unable to 
explain 3D motions between two image planes, but provides superior 
stability when compared to projective transformation. 
In near-flat  areas,  the 
errors of object height projections are usually negligible. Thus, the 
affine  transformation  can  be  directly  implemented.  
For our sea ice imagery, we take advantage of the 
large distance to the scene and the generally planar terrain surface and
 adopt a model which assumes equal distance to each object in the image. This 
 allows the use of the more stable  
utilize affine transformation to translate the image coordinates.
If a UAV  were able to achieve a perfect downward-facing camera survey over flat terrain, a similarity transfom 
which is defined by rotation, translation, and scale would be sufficient to describe the 
motion between images, though this is often not practical. 
Even simpler applications, such as a panorama building witha rotating camera, 
only need simple 2D translation and rotation to align images.  

Homography can be determined by various methods like RANSAC, Least Means Squares, or a Hough Transform. 
In this paper, we determine homography using  RANSAC or "RANdom SAmple 
Consensus. RANSAC \cite{RANSAC} is an iterative parameter estimation technique 
that is widely in image registration problems.  It works by repeatedly taking random 
pixel matches from a set of data, fitting them to a proposed model, and then measuring 
the error from that model against all matches. 
For each proposed model, all of the matches are classified as either inliers or outliers by calculating the residual 
error  with respect to the model.  
This process is performed until a maximum 
iteration is hit or a model is estimated which meets a stopping criteria. Then 
the best model (with the least residual) is fit for all of the data. 


The homography calculated from RANSAC 
is used to warp the image onto the reference plane which serves as the compositing surface. For our work, which involves a  largely translational camera,
a planar compositing 
surface is most appropriate. A planar surface preserves perspective so that straight lines 
in the scene are stright in the mosaic. Other applications, such as with a rotational camera as in a typical panorama is to use a cylindrical or spherical 
projection so that the surface is curved like a sphere. 


\subsection{Blending}

A pixel in the final stitched mosaic corresponds to a single point in the scene, 
but because that scene was captured at different times from a different
viewpoint, the pixels from the images being composited at that pixel may be 
different. 
This is especially true when neighboring images in a mosaic are captured at different points in time as illumination can vary significantly.  
To produce a seamless output mosaic, we must determine the best 
pixel value for each image in the overlapping region and for the pixels that border in these regions. There are many different approaches to selecting 
these pixel values and blending the competing images. 

One of the most popular methods for blending images together is through the 
use of multi-resolution splines \cite{Burt83}. This is called Pyramidal blending in which different bands of frequency in each of the images are mixed at different weights. The low-frequency color variations 
are blended smoothly, but the high frequency textures are blended quickly using 
each images Laplacian pyramid. In our case, homogenous regions like ice sheets 
have low spatial frequency and are combined across a wide region.
Strong color changes, such as ice-water transitions show high spatial frequency 
and are fused over a small area. 




\subsection{Related Work}
\label{sec:related}
In this section we take a look at recent developments in image mapping with moving vehicles. Although this field has been widely studied for many years, some problems remain elusive. 

These include but are not limited to: 

\begin{itemize}
    \item{Recovering from poor overlap}
    \item{Matching images with repeating, indistinguishable, or dynamic features}
    \item{Matching scenes under different conditions (day and night or change in seasons) \cite{Valgreen10} 
    \item{Resolution and quality of output mosaic}
    \item{Identifying objects in the scene}
\end{itemize}

In \cite{Prathap16}, Prathap gives a thorough review of the current state of mosaicing. 

In addition to image mosaicing are two closely related objectives:
Structure from Motion (SFM) and Simultaneous Localization and Mapping (SLAM). Structure 
from motion \cite{Pollefeys98} is an extension of mosaicing that  produced  
3D geometry of the scene and the motion of the moving camera 
\cite{Morimoto97, Shum-3D98}.  
SFM have difficult scaling to long image sequences and fail without quality overlapping  points 
for triangulation. SFM is generally performed after all of the images have been collected, but SLAM performs a similar task but in real time. This allows a moving vehicle 
to both build a map of its environment and use this information for localization 
with an implementation of recursive Bayes estimation, usually an varient of a Kalman Filter or a Particle Filter \cite{MonoSLAM}. SLAM is currently irrelevent for our goal of
providing good estimates of sea ice in a setting which our platform neccessarily 
has good localization through GPS. 

\subsubsection{Improving Feature Selection}

In 2010, Botterill proposed using Bag-of-Words representation of images 
to reduce the search space for overlapping images. This provides a relatively cheap 
approach to reduce the search spae for finding wide-baseline correspondences \cite{Botterill10}


\subsubsection{Sensor Integration}

Much of the current work in UAV image mosaicing relates to using metadata from 
the vehicle flight to improve mosaic output or SLAM and SFM efforts to determine vehicle location. For the purpose of this paper, we'll concentrate on advance made in improving the output map and not on localization. 

\begin{equation}
    label{eq:ekf}
x_k = f(x_{k-1}, u_k) + w_k
z_k = h(x_k) + v_k
\end{equation}
The metadata provided by the vehicle's navigation sensors can provide low-resolution clues to reduce the search space for matching correspondences. In \cite{Cheng10}, Xing et al  divide image sequences from UAV flight into small groups of 
overlapping images to perform local optimization on. This optimization is based on the Extended Kalman 
Filter (EKF) and corrects homographies in a local area before performing a 
global optimization step on the entire image set. 
The EKF, developed for the Apollo Project \cite{NASA} and defined in Equation \ref{eq:ekf} extends the Kalman Filter to nonlinear systems by linearing around  a working point. In the case of tracking features between frames, it allows us to estimate the location of these features given information about the state of the camera which is imperfectly measured by the UAV's sensors.  
Civera et al \cite{Civera09} also 
use an EKF to improve the mosaic output from a rotating camera, but describes the process  from 
a SLAM perspective. This paper provided  real-time, drift free mosaicing, though it was only demonstrated in a small scene. 

Proposed a real-time approach of mosaicing images from UAVs for disaster 
response. First, the images were downsampled and roughly aligned using only 
GPS and IMU data, then as bandwidth allowed, the alignment was refined using 
feature based methods \cite{TODO}


\subsubsection{Featureless Scenes}
Prasad, et al investigated the problem of developing mosaics in quadcopter surveys 
with gaps in features using IMU information to improve convergence time.
The approach was demonstrated on building walls with only small regions (paintings) 
that provided keypoints\cite{Prasad16} 


Bay et al \cite{Bay05} matched scenes using line segments extracted with a Canny 
edge detector  in addition to descriptors to match homogenous scenes like 
architectural interiors. Line segments are able to convey geometrical and 
topological information that made wide-baseline matching more reliable. 

Caballero uses the match success between images to limit the homography model selection. Using a simpler model if fewer matches are available \cite{Caballero07}.  

Registration \cite{Barnea72, Shum-local98, Sawhney99}


\cite{Schaffalitzky01}

\subsubsection{Dynamic Scenes}
Developing a mosaic from a scene in which images are captured at disparate times means that moving objects within the scene may cause matching errors when features are matched between the moving object from multiple image frames. This is also a problem that presents itself in our dataset as motion from water covered regions in our imagery in the form of waves or ripples could cause false matching.  

There are many methods for mosaicing a scene with dynamics in the static mosaic
\cite{Fitzgibbon01, Rav-Acha07, Davis98, Uyttendaele01}. 
Some approaches eliminate all dynamic information in the scene, while others 
attempt to encapsulate the changes between images by overlaying the movement 
into the mosaic. 
However, our approach was to use a simpler, masking scheme to bar the feature 
extraction algorithm from extracting keypoints from the dynamic portions of our 
images. This also allows us to estimate sea ice coverage effectively solving two problems at once, so this portion of the project is further covered in \ref{chap:class}. Our approach is similar to that of Vousdoukas in \cite{Vousdoukas11} to mask sea and sea foam
from shoreline images captured form an UAV. Vousdoukas, however,  
manually selected pixels associated with the 
sea foam and performed simple thresholding based on similar intensities before 
using SIFT descriptors to mosaic the coastline.

\subsection{Optimization}
Strictly local methods, which match neighboring images until until there is 
only the final mosaic left, tend to accumulate errors 
resulting in distortion \cite{Lin07}.
A globally consistent mosaic should use all overlap information to produce 
the best possible representation of the scene. 
Typical approaches to improve global consistency will either work by adding
incremental links to the mosaics as in a real-time 
approach, or by solving for all of the images at once to distribute 
misregistration errors after all images have been collected. 

Historically, the most broadly used global refinement method solves for all
images transforms at once as a final computationally expensive step offline. 
This approach requires that  a specific feature match to the same point 
in the final mosaic from all images. This is often implemented through a process 
called \emph{bundle adjustment} \cite{bundleadjust} which minimizes reprojection 
error a using nonlinear 
least squares algorithm to solve for transform parameters that minimize the 
distance between corresponding features in the mosaic. 
The process of simultaneously 
aligning all of the images can be slow 
to converge, but genererally results in a more correct mosaic \cite{Pizarro-thesis03}. 


However, many modern image alignment problems prefer real-time or near real-time 
approaches and there has been recent work in this area. 
Lin \cite{Lin07} proposed to register 
each frame in an image sequence to its previous image that had been 
previously referenced to a lower-scale map of the region. He was able to use
low-resolution satellite imagery to improve global alignment high-resolution 
UAV imagery in near real-time.  This resolves scale and rotation 
differences between the images before solving for pixel wise matching which 
prevents errors between frames from accumulating. 
Although an interesting approach, 
satellite imagery generally updates too slowly 
to be useful for the dynamic state of sea ice. 




\section{Sea Ice Detection}

Many governments and organizations dedicate significant resources to providing 
timely assessments and maps of sea ice coverage to ensure safety 
in these waters and to gain greater understanding of the earth's processes. 
One of the most public datasets for sea ice is produced by the Canadian Ice Service. 
Their team of experts provide regular ice assesments over the Artic.
The agency also produces daily ice charts, which are an estimate of the current ice 
conditions based on satelite data that is augmented with aerial and ship data. 

The Manual of Ice (MANICE) provided by the Canadian Ice Service, describes various 
types of ice discussed throughout this paper \cite{CanadianIceService}. Below we provide 
a brief introduction to several forms of ice relevent to our experiments.  
\begin{itemize}
    \item{\textbf{Pancake ice} is defined as predominetly 
circular pieces of ice the is $30 cm$ to $3 m$ in diameter. The edges are typically 
raised from the pieces striking against one another. See Figure \ref{fig:pancake} for reference. }
\item{An \textbf{ice cake} is a flat piece that is less than 20 m across. See Figure \ref{fig:cake} for reference. }
\item{An \textbf{ice floe} is a flat piece of ice that is 20 m or more across. Small floes are 
    less than 100 m and giant floes are those pieces that are greater than 10 km across. }
\item{\textbf{Slush} is recently formed ice which is saturated with water and composed of crystals that are only weakly frozen together.} 
\item{\textbf{Glacial ice} is ice that orginated from a glacier. When glacial ice is floating on the sea, it is broken up into \textbf{icebergs},  \textbf{bergy bits}, and \textbf{growlers} which describes level at which the ice floats in the water.  Icebergs are the massive pieces of ice that protude more than 5 m above the sea. Bergy bits are pieces of icebergs which show 1 to 5 m while growler  show less than 1 m above the water line}. 
\item{\textbf{Multi-year ice} has survived at least two summer's melt that is nearly salt-free and blue in color.}
\end{itemize}

Periods of poor visibility due to fog and snow make detecting ice visually dificult, even in the summer months which experience constant daylight. 
For ships transiting in polar regions, it is important to differentiate 
between first-year ice and multi-year ice which can be dangerous.  

New ice tends to have higher concentrations of salt trapped in empty cavities between ice crystals, that makes it less dense. After several years, the ice becomes much thicker and loses much of the salt to runoff \cite{CanadianIceService}. 
Growlers and bergy bits can be especially dangerous for even for hardened vessels as they are hard to spot with marine radar.  
Conventional Marine radar can be used to detect icebergs usually within $1 km$ of the ship, but this sensor is unable to measure ice
floes and their movement on a large scale. Though recently developed high-resolution marine radar address this problem at close range \cite{CanadianIceService}, most radars still have difficulty detecting small floes of multiyear ice or growlers which float low in the water. 



When assessing ice, Sephton et al. in \cite{Sephton94} outlines the following qualities that are of significance:
\begin{itemize}
    \item{The type, thickness,  position, and size of discrete ice floes}
    \item{Total concentration of sea ice as defined by the fraction of the surface area covered by ice over the measured region. }
    \item{Local and global movement of ice floes}
\end{itemize}



\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{pancakes.jpg}
        \captionsetup{width=0.9\linewidth}
        \captionof{figure}{A close photo of pancakes, small pieces of ice with raised edges from bumping one another. }
         \label{fig:pancake}
     \end{minipage}%
     \begin{minipage}{.5\textwidth}
         \centering
        \includegraphics[width=.9\linewidth]{cakes.jpg}
        \captionsetup{width=0.9\linewidth}
        \captionof{figure}{An example of a large ice cakes. For reference, the pictured ship, the R/V Sikuliaq, is 79.6 m in length.}
             \label{fig:cake}
      \end{minipage}
\end{figure}


To quantify ice floes it is advantageous to first segment the ice from the 
surounding water and neighboring floes. This has been well studied for SAR 
imagery.
Much of the published work concerning the segmentation of sea 
ice from water is performed on data collected 
from synthetic apertaure radar images collected from satellites. 
While this work is interesting 
and encouraging, the dataset from the low-resolution SAR imagery and our high 
resolution images are different enough that there exists marginal overlap in 
performance for comparison. Nevertheless, the approaches used for SAR segmentation will be 
covered in the following section to provide background on approaches that have 
been proven to work on a similar dataset. 


\subsection{High Altitude Sea Ice Classification} 
\label{sec:highalt}

\begin{figure}
\label{fig:eggcode}
\centering
\includegraphics[width=.9\linewidth, height=0.6\linewidth]{egg_code.jpg}
  \captionsetup{width=.9\linewidth}
  \captionof{figure}{
  Ice estimates over the Hudson Bay region of Canada produced by the Canadian Ice Service \cite{CanadianIceService}. These reports are produced daily in season, subject to atmospheric and sea conditions. }
\end{figure}


By far the most common technique for remotely monitoring sea ice is through data obtained 
by satellites.  In addition to passive sensors,
sea ice is also frequently monitored  by active sensors which can 
build models of the physical geometry of the ice. 
Synthetic Aperture Radars (SARs) 
are a form of active microwave operated at around 5.3 Ghz
 commonly used to determine the 
age and thickness of ice,  and differentiate between ice and water.
SAR is advantageous because it can operate without natural light and despite 
harsh weath conditions such as clouds or rain. Considering the 
poles experience long periods of darkness during peak ice cover, this is an important attribute of this mechanism. 
RADARSAT-1, launched in 1995, is the main SAR mission gathering data today \cite{CanadianIceService}.
Howveer, SAR datasets are typically captured  at a  relatively low resolution when compared to our data, with 1 pixel represeting 
50 meters on the earth's surface with the data often plagued with speckle noise 
 \cite{CanadianIceService, Li15, Xu14, Clausi08}. Speckle noise is the 
 result of random reduction in the radar's return signal caused by interference 
 from objects on the scale of the wavelength. 

 The sea ice maps produced by the Canadian Ice Agency are labeled by homogenous ice regions with an  
"egg code" symbol (see Figure \ref{fig:eggcode} manually by sea ice experts). These egg codes summarize the region's ice characteristics with details 
such as ice floe, ice concentration, and ice types but are far larger than the pixel-wise region  \cite{Clausi08, CanadianIceService}. Collecting and labeling this data is a demanding  and expensive task and thus many attempts  have 
been made to automate the process, though none have been successfully implemented for public release \cite{Clausi03, Clausi08}. 
 In the following paragraph, we will summarize recent approaches to determine 
 the ice/water boundary in SAR imagery where ground-truth region labels provided by the Canadian Ice Service in most cases. 

SAR sea ice segmentation approaches are typically pixel-based or 
texture-based and a simple grey theshold has been utilized quite successfully to seperate ice and water \cite{Haverkamp93}. The success of this thresholding is  because of the well-defined bimodal nature of sea ice imagery, 
however, this simple approach is also susecptable to speckle noise. 
Other pixel-based methods include mixture models \cite{Samadani95} and k-means
clustering \cite{Remund98}.
Markov Random Fields, described in \cite{Li95MRF} are used by Deng and Clausi in \cite{Deng04, Deng05}, Yang and Clausi in \cite{Yang09} and by Clausi in \cite{Clausi01}. MRFs base the pixel segmentation class of each pixel on its neighboring pixel values and on the conditional segment probabilities and improve segmentation performance, especially in the presence of speckle noise. 
In \cite{Clausi03}, Clausi considers practical segmentation of SAR sea ice for the Canadian Ice Agency and demonstrates segmentation using a finite gamma mixture model, K-means clustering, binary hierarchical K-means iterative Fisher, and Markov random field modeling and shows good results with the gamma mixture model followed by a MRF label model to discriminate between ice and water. 

Texture-based algorithms, as opposed to pixel-based algorithms, use texture features which are functions of neighboring pixels. This provides some robustness against speckle noise. Many practitioners  use 
the grey-level co-occurence matrix (GLCM) which was introduced by Haralick et al. \cite{Haralick73}. GLCMs were used extensively by others including Soh in \cite{Soh98} and Clausi \cite{Clausi01, Clausi03} on SAR data with success.
Xu et al. \cite{Xu14} use kernal principle component analysis (KPCA) 
with local texture feature model to segment sea ice from SAR data efficiently despite speckle noise for operational segmentation. 
In conjunction with the Canadian Ice Service, Clausi presented in \cite{Clausi08} MAp-Guided Ice 
Classification system or (MAGIC) to interpret SAR sea ice images with ice charts. 
This software estimates pixel level classification allowing the user to specify polygons 
on the earth's surface. MAGIC uses the watershed \cite{watershed} algorithm to segment the SAR image into several regions and performs supervised classification using a MRF. 
In 2014, Leigh et al. \cite{Leigh14} extended MAGIC's classifiation with a pixel-based support vector machine that uses gray-level cooccurence texture and backscatter. 


Several new approaches utilize Convolutional Neural Networks, or CNNs for ice segmentation and classification. CNNs 
have recently dominated many computer vision applications, outperforming methods 
which rely on hand-crafted features  \cite{Imagenet} and were also shown to perform well for remotely sensed images by. Basu et al. \cite{Basu15}. 
In an innovative SAR paper, Li et al. in \cite{Li15} utilized the egg codes from 
the Canadian Ice Service to label ice pixels using a \emph{Learning from 
Label Proportions} (LFLP) approach to model patch-level ice vs water classification using the egg code percentages and overcoming a limitation of the labeling.  
 Wang et. al \cite{Wang16} demonstrated pixel-wise segmentation 
to with absolute mean error of less than 10\%  when compared to human experts using deep CNNs in a small study of SAR images. 

%Manual of ICE
%https://www.ec.gc.ca/glaces-ice/default.asp?lang=En&n=2CE448E2-1


\subsection{Low-Altitude Sea Ice Classification}
UAVs are rapidly becoming the medium of choice for perfoming sea ice reconaissance as indicated several recent publications. 
To understand the importance of UAVs in the space, we'll first give some background on the technology that working ships currently employ for situational awareness and movement. 
Modern working ships typically utilize a computer control systems called \emph{dynamic positioning} (DP) for navigation and station keeping. These systems  utilize a variety of sensors to maintain position usually including GPS, IMUs, wind sensors, and current sensors like Doppler Velocity Loggers (DVLs) to calculate the ship's current position and the necessary accuation of the  propulsion and steering systems to move to a prescribed location.
Dynamic Positioning is a vital part of modern ship navigation and is especially important for drilling or scientific operations in which the ship must maintain a static position for long periods of time. Information about real-time ice movement for ships working in these water is essential to the proper function of DP systems, but lacking in most systems. 

In 2014, Skjetne et al.  proposed utilizing UAVs for drift prediction to aid ships 
attempting to maintain Dynamic Positioning (DP) in ice-laden waters \cite{Skjetne14}. Skjetne divided ice monitoring into two problems; the first is 
the one that we will tackle, dynamic sea ice monitoring to track the movement 
of large bodies of ice sheets. The second problem is that of tracking large icebergs 
which involves recognizing icebergs on temporally spaced observations and tracking 
their movement.  
Skjetne worked on path planning for UAVs with simulated icebergs. He also proposed a method 
of monitoring sea ice concentration by thresholding grayscale imagery of sea ice 
\cite{SkjetneTODO}. 
He also estimated floe size, by finding closed boundaries around individual 
ice flows using the Gradient Vector Flow Snake algorithm to detect closed curves \cite{SkjetneTODO2}. 


Lagunov et al. \cite{Lagunov14} describes the value of sea ice monitoring and provided an 
overview of useful sensor platforms including the UAV and provides a thorough description 
of the many considerations of sensors and operational constraints involved in 
deploying a UAV for ice observation. 
This paper also demonstrates the usefullness of the platform with a microwave sensor 
and camera over an ice covered river, but did not cover problems related to organizing 
the imagery or automatically evaluating ice coverage.

Billington proposed the use of UAVs for shape estimation of icebergs at sea
with UAVs equiped with imaging systems and provided a description of requirements 
for operations \cite{Billington15}. He proposed a neural network to perform 
shape estimation of the submerged portion of the iceberg based on the observed 
surface and demostrated his effort on a small sample size that made it difficult to 
evaluate. 


Zhang et al. has published a number of papers on sea ice segmentation and floe 
detection. In \cite{ Zhang13}, Zhang operates on high resolution images by first 
determining seperate ice floes using the Otsu \cite{OTSU} threshold algorithm to 
binarize the image. Then a watershed segmentation is used to segment indiviual ice floes. 
Both \cite{Blunt12, Zhang13} use watershed algorithms to segment ice images,
but found that oversegmentation reduced the accuracy.
Over-segmentation is caused by the structure of the watershed algorithm. The
watershed algorithm depends on the local minima  for the starting point. When
there are multiple local minima in the 
image, like is often the case in sea ice imagery, over-segmentation occurs. 
Zhang, in \cite{Zhang13} corrects this for the watershed algorithm
by using a combination of concave detection and neighboring region merging
to isolate floes. Blunt, in 
 \cite{Blunt12} simply removes over-segmented 
lines manually. 

In \cite{Zhang14} discussed Otsu thresholding and k-means clustering
\cite{MacQueen67} and demonstrated how k-means could be used to better segment images with several types of ice and water. Zhang then demonstrates segmentation of floes using a Gradient Vector Flow Snake Algorithm with a circular shape that deforms to 
the floe boundary on the binary image. 
Zhang then uses morphological cleaning to ensure completeness of the ice floe so 
that smaller foes contained in larger floes are removed. 
In \cite{Zhang15floe}, Zhang improves segmentation of ice floe boundaries, which can be difficult to identify when floes are touching again using the Gradient Vector Flow Snake Algorithm, but this time separating the image into 
different layers based on ice type and sizes. 

Flaten discussed mosaicing sea ice images for the purpose of UAV navigation in \cite{Flaten-thesis15}, 
but, was unable to demonstrate results using field data. 
This project also attempted to provide ice density 
estimation by segmenting the image to produce an occupancy grid 
map estimation of sea ice density. 
Flaten demonstrated Otsu thresholding segmentation on several photos collected from a UAV
and from simulated imagery.



\chapter{Mosaic Construction}
In this chapter, we describe the process we've taken to build a robust system for mosaicing real ice imagery. These images are often difficult to find overlap because the images contain large spaces in which there are few discernable features.  To overcome this, we show how we can utilize sensor data from the sensors on the UAV. 
\label{chap:features}
TODO Introduction and overview



\subsection{Image Overlap Search Space}
Knowing the location of the viewpoint from 
which the images were captured can reduce the search space for overlap. This 
may include using a GPS location or timestamp of the photo with some knowledge 
of the motion of the camera. Low cost sensors, particularly those used on 
lightweight UAVs, generally do not provide accurate enough estimates of position 
to give per-pixel allignment for high-resolution images at close range (<100m). 

Several practitioners of image mosaicing have demonstrated 
ways in which to use sensor information to reduce the search space for overlapping 
images in applications. 

\section{Feature Evaluation}
Like, most modern approaches, we register images by selecting key 
features from each image 
and attempting to relate them to other images in the scene. 
There exist a wide variety of feature extraction and description
techniques that enable matching between visual correspondences despite 
rotation, scaling, or environmental changes such as lighting 
noise. 
In addition to invariance, a high number of detected features is also desireable
to improve matching. 
Keypoints should have high information content as measured by 
repeatibility rate and information content. 

Feature detectors rely on the existence of edges, corners, or  blobs in 
the image to match images. If none occur, then no features can be detected. 
Feature detecting algorithms tend to have different strengths and weaknesses as 
discussed in several studies \cite{Tuytelaars08, Mikolajczyk05, Heinly12, Bekele13}. 
Bekele \cite{Bekele13} in particular proved a good 
overview of popular feature descriptors and 
detectors and their performance which guided our evaluation on sea ice dataset.
Describing keypoints is difficult in ice surveys 
because of the lack of distinguishable features on images that contain 
only ice or only water. All ice images often contain a high degree of repetitive 
simularity which means even distinctive features can be incorectly matched with 
non-overlapping images. 

Heinly showed that the descriptors should be adapted to the transofrmations 
present in the data \cite{Heinly12}.

Equation \ref{eq:putativematches} was introduced by \cite{Heinly12}.

% Quantifying a match
In the following sections, feature detectors and descriptors are discussed and 
evaluated on our dataset based on a number of points: 

%where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).
%Least median of squares (LMedS) is used for outlier rejection and a M-Estimator to compute the final result. This model is used if more than the 65\% of the matches are successfully tracked.
%. Affine homography. If the percentage of success in thetrackingstepisbetween40\% and65\%,thenthe LMedS isnotused,giventhereductioninthenumber ofmatches.A relaxedM-Estimator(softpenalization) is carried out to compute the model.
%
%Euclidean homography. If the percentage is below 40\%, the set of data is too noisy and small to apply non- linear minimizations. The model is computed using least-squares.
%\cite{Caballero07}




\begin{itemize}
    \item{Average number of keypoints}
    \item{Precision of matches}
    \item{Recall of matches} 
    \item{Average number of good matches}
\end{itemize}

\begin{equation}
recall = \frac{number of correct matches }{number of correspondences}
\label{eq:recall}
\end{equation}

\begin{equation}
1-precision = \frac{number of false matches }{number of matches}
\label{eq:precision}
\end{equation}

\begin{equation}
putative match ratio = \frac{number of putative matches }{maximum possible matches}
\label{eq:putativematches}
\end{equation}

\begin{figure}
\label{fig:rotate_results}
\centering
\includegraphics[width=\linewidth, height=.6\paperheight]{rotate.png}
        \captionsetup{width=.9\linewidth}
        \captionof{figure}{
A comparison of different feature descriptors on two datasets of 60 images randomly selected over a variety of conditions. The dataset labeled "good features" contained images with strong corners and lines, while the dataset labeled "bad features" contained images with only snow or ice and not significant features. Each image was subjected to a change in rotation and scored on its ability to match to the original image using a brute-force matcher.  }
\end{figure}

\begin{figure}
\label{fig:scale_results}
\centering
\includegraphics[width=\linewidth, height=.6\paperheight]{Scale.png}
        \captionsetup{width=.9\linewidth}
        \captionof{figure}{
A comparison of different feature descriptors on two datasets of 60 images randomly selected over a variety of conditions. The dataset labeled "good features" contained images with strong corners and lines, while the dataset labeled "bad features" contained images with only snow or ice and not significant features. Each image was subjected to a change in scale and scored on its ability to match to the original image using a brute-force matcher.  }
\end{figure}


Today, the most common technique for monitoring sea ice is through data obtained 
by satellites. 
%A  perfect descriptor would give a recall equal to 1 for any precision

Putative matches, proposed by Heinly, allows us to measure the 
selectivity of the descriptor. 
precision is also the inlier ratio (ransac). The precision greatly influences the performance of robust estimation techniques like RANSAC. RANSAC's runtime increases exponentially as the inlier ratio decreases \cite{Heinly12}. 

%Percent of matches - quotient of dividing matches count on the minimum of keypoints count on two frames in percents.
%Percent of matches - quotient of dividing matches count on the minimum of keypoints count on two frames in percents.
%Percent of correct matches - quotient of dividing correct matches count on total matches count in percents.
%Matching ratio - percent of matches * percent of correct matches. In all charts i will use "Matching ratio" ( in percents) value for Y-axis.


Parameters in each of the detectors are 
tuned so that 4000 features are detected in the reference image.  
If more than 4000 features are detected in the image, only the first 4000 
are used in the evaluation.  

TODO:
plot featureless comparisons for each of the descriptors at one rotation

\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{no_features.jpg}
        \captionsetup{width=0.9\linewidth}
        \captionof{figure}{A typical image of sea ice captured from a UAV with few features and homogoneous texture. }
         \label{fig:no_features}
     \end{minipage}%
     \begin{minipage}{.5\textwidth}
         \centering
        \includegraphics[width=.9\linewidth]{good_features.jpg}
        \captionsetup{width=0.9\linewidth}
        \captionof{figure}{A sea ice image with many corners and distinctive  features to use for feature matching. }
             \label{fig:good_features}
      \end{minipage}
\end{figure}


This approach utilizes 
the knowledge that the UAV captured images in a spatial order as it was flying, 
so we attempt to match images with neighboring timestamps first.
This sequential approach is not necessary, but will decrease convergence time 
as it limits the number images that must be searched in a typical mission. 
Local motion around each feature point is expected to be mostly 
translational with minimal roll and pitch of the camera. 


%Features with a low success ratio in matching (about 0.5) are deleted from the 
%map if at least 10 matches have been attempted. Mapmaintence allows deleting of non 
%trackable features - for instance those captured on moving objects such as 
%waves or people. 

%The percentage of successful matches obtained by the point tracker is used to have an estimation about the level ofthehierarchywherethehomographycomputationshould start.Thesepercentagethresholdswereobtainedempirically byprocessinghundredsofaerialimages.Eachlevelinvolves thefollowingdifferentsteps:
%



Our approach utilizes the open 
source tool, Enblend, that uses a multi-resolution spline to blend images 
together using multi-resolution splines and Laplacian pyramids \cite{enblend, 
Burt83}. 

\chapter{Ice Segmentation and Classification}

Segmentation of an image into pixel classes can be thought of as either a 
supervised problem which requires training a set of classfiers from data  
which has been labelled at the pixel level or an unsupervised problem works without labels. For our problem of high-resolution ice imagery, there does not exist to our knowledge a large database of labeled pixels from which to train in a supervised manner. Instead, we formulate our problem as unsupervised classification from which we partition the image into coherent regions according 
to class-independent cues \cite{Csurka13}. 

Labelled as approximate ground truth by human experts. 

We utilize the Jaccard index \cite{Jaccard1901} (also known as the Jaccard Similarity Coefficient), 
a measure of the overlap ratio to determine the similarity between our calculated classification and the labelled image classes. This measure is defined by the 
size of the intersection divided by the size of the union of the dataset. 
Thus, the Jaccard index is zero if there are no correctly classified pixels and 1 if all pixels are assigned the correct class. 



\begin{equation}
    \label{eq:jaccard}
    J(A,B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation}
\chapter{Description of Approach}



\chapter{Ice Density Estimation}
\label{chap:density}

\chapter{Experiments and Results}

\label{chap:results}
\section{Experimental Setup}
Data used for this study was collected in the Beaufort Sea, approximately 
350 km north of Alaska's northern coastline. The UAV perfomed 12 flights 
over 5 days from various locations at altitudes varying from TODO m to TODO m. 

TODO: total number of photos, total coverage area
\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth,height=0.6\linewidth]{uav_snow_d.jpg}
        \captionsetup{width=0.9\linewidth}
        \captionof{figure}{The Phantom FPV Flying Wing EPO UAV used to gather data over sea ice. This vehicle was equipped with a TODO camera for capturing images. }
         \label{fig:ouruav}
     \end{minipage}%
     \begin{minipage}{.5\textwidth}
         \centering
        \includegraphics[width=.9\linewidth,height=0.6\linewidth]{launch_uav_d.jpg}
        \captionsetup{width=0.9\linewidth}
        \captionof{figure}{UAVs have small space requirments for both storage and flight, making them ideal scouts for working off of a ship.}
             \label{fig:shipscout}
      \end{minipage}
\end{figure}


\subsection{Flight Vehicle}
The vehicle used in our experiments is a fixed-wing UAV pictured in Figure \ref{ouruav}. This vehicle has an endurance of 
approximately TODO. The vehicle has autonomous flight system which follows 
a specified trajectory over the earth's surface at a prescribed altitude. 

\subsection{Sensor Payload}
The vehicle caries a payload of low-cost IMU, GPS receiver and a 
downwards-mounted color camera. The GPS receiver computes earth-referenced position at TODO Hz with an accuracy of TODO. Photos are captured at a resolution 
of 6000x4000 pixels at a rate of TODO Hz. An onboard computer logs sensor data 
which is processed after the flight. 
Lens used is a Sony E  with a focal length of $20 mm$. 
Camera is a Sony ICLE-5100.
\section{Results}

\section{Classification}
As discussed in Chapter \ref{chap:classification}, TODO ice images were 
labeled by hand to provide ground truth for the classification results. 

Cross validation
Confusion Matrix


\subsection{Validation - Synthetic Mosaic}
A synthetic survey is generated from a single image by dividing the image into
a grid overlapping sub-images.  This provides a planar scene with an ideal 
pinhole camera. Simple translations produce the resulting image. Another test 
introduced rotation and scaling into each sub-image. 
demo 
translation - minimal overlap, detected that there was no rotation or scaling.
rotation   - what angle limit?
-- show plot of actual angle of rotation vs calulated
scaling - images should be scaled to size of smallest before stitching
-- show plot of actual scaling vs calculated
demo 
\subsection{Flight Map}

\chapter{Conclusion and Future Work}
\label{chap:conclusion}
This paper has presented a framework for surveying sea ice with a UAV equipped with a color camera. This work opens the world of sea ice reconissance and 
research with its unique processing capability at a low cost. A classification algorithm is implemented to classifify sea ice and a mosaicing routine is 
introduced to produce high-quality low-altitude maps. 

The final classification results indicate that the classifier performs well 
when distinguishing between sea and ice and between ice types.. 

Adaptive collection techniques to allow the UAV to search for areas that is has
not covered, or to prioritize coverage of interesting areas, such as regions with 
large icebergs. 
Iceberg elevation or size calculations

\pagebreak{}

\bibliography{biblio}
\begin{vita}
This should be a one-page short vita.

There can be more paragraphs.\end{vita}
\end{document}



%
%\begin{figure}[H]
%\noindent \begin{centering}
%\framebox{\begin{minipage}[t]{1\columnwidth}%
%\textbackslash{}documentclass{[}12pt,english{]}\{report\}
%
%\textbackslash{}usepackage\{UTSAthesis\}
%
%... use other packages ...
%
%\textbackslash{}begin\{document\}
%
%\textbackslash{}committee\{... \}
%
%\textbackslash{}informationitems\{... \}
%
%\textbackslash{}thesiscopyright\{...\}
%
%\textbackslash{}dedication\{\textbackslash{}emph\{I would like to
%dedicate this thesis/dissertation to ...\}\}
%
%\textbackslash{}title\{\textbackslash{}textbf\{First line\}\textbackslash{}\textbackslash{}
%\textbackslash{}textbf\{second line \}...\}
%
%\textbackslash{}author\{...\} 
%\textbackslash{}maketitle 
%\textbackslash{}begin\{acknowledgements\} ... \textbackslash{}end\{acknowledgements\}
%\textbackslash{}begin\{abstract\} ... \textbackslash{}end\{abstract\}
%\textbackslash{}newpage 
%\textbackslash{}pagenumbering \{arabic\} 
%\textbackslash{}setcounter \{page\}\{1\} 
%\textbackslash{}pagestyle\{plain\}
%\textbackslash{}chapter\{...\} \% or \textbackslash{}include\{chap3\}
%...
%\textbackslash{}singlespace
%\textbackslash{}bibliographystyle\{...\} 
%\textbackslash{}bibliography\{...\}
%\textbackslash{}begin\{vita\}...\textbackslash{}end\{vita\}%
%\end{minipage}}
%\par\end{centering}
%\caption{Structure of a thesis \protect\LaTeX{} file\label{fig:Structure-of-thesis}}
%\end{figure}
%
%
%The following commands are defined in UTSAthesis.sty and should be
%used in the order suggested in Fig. \ref{fig:Structure-of-thesis}
%to provide required format information.
%\begin{itemize}
%\item \textbackslash{}title\{Thesis Title\}. This can contain multiple lines.
%Use ``\textbackslash{}\textbackslash{}'' to go to the next line.
%\item \textbackslash{}author\{Name of Thesis Author\}
%\item \textbackslash{}thesiscopyright\{Optional Copyright Statement\} 
%\item \textbackslash{}dedication\{Optional Dedication\} 
%\item Either \textbackslash{}committee\{Supervisor Name, Degree\}\{Co-Supervisor
%or Committee B Name, Degree\}\{Committee C Name, Degree\}\{Committee
%D Name, Degree\}\{Committee E Name, Degree\} or the following commands
%separately.
%
%\begin{itemize}
%\item \textbackslash{}supervisor\{Supervisor Name, Degree\} 
%\item \textbackslash{}cosupervisor\{Co-Supervisor Name, Degree\} or \textbackslash{}committeeB\{Committe
%member B Name, Degree\} 
%\item \textbackslash{}committeeC\{Committe member C, Degree\} 
%\item \textbackslash{}committeeD\{Committe member D, Degree\} 
%\item \textbackslash{}committeeE\{Committe member E, Degree\}
%\end{itemize}
%\item Either \textbackslash{}informationitems\{Full Name of Degree\}\{Short
%Name of Degree\}\{Full Name of Department\}\{Full Name of College\}\{Month
%of Thesis\}\{Year of Thesis\} or use the following commands separately.
%
%\begin{itemize}
%\item \textbackslash{}degree\{Full Degree Name\} 
%\item \textbackslash{}degreeshort\{Short Degree Name\} 
%\item \textbackslash{}department\{Department Name\} 
%\item \textbackslash{}college\{College Name\} 
%\item \textbackslash{}thesismonth\{Month\} 
%\item \textbackslash{}thesisyear\{Year\} 
%\end{itemize}
%\item \textbackslash{}maketitle is the command to produce the signature
%page, copyright page, dedication page, and the title page. The position
%of this command is important. 
%\item \textbackslash{}begin\{acknowledgements\}
%
%
%People, organization, supports that you want to thank for 
%
%
%\textbackslash{}end\{acknowledgements\}
%
%\item \textbackslash{}begin\{abstract\}
%
%
%The abstract starts here. Should within one page.
%
%
%\textbackslash{}end\{abstract\} 
%
%\item The thesis/dissertation should then continue with chapters, appendixes,
%references. Before the first chapter, it is necessary to set Arabic
%page number. If the thesis/dissertation is long, it may be better
%to place chapters into separate \LaTeX{} files and include these sub-files
%using \textbackslash{}include\{\} command.
%\item \textbackslash{}begin\{vita\}
%
%
%The last item is a one-page curriculum vita
%
%
%\textbackslash{}end\{vita\}
%
%\end{itemize}
%
%\subsection{Produce the Outcome}
%
%To produce the pdf version of the thesis/dissertation, run pdflatex
%and bibtex.
%
%
%\section{The utsathesis.layout Package}
%
%The utsathesis.layout is an \LyX{} layout that provides a \LyX{} document
%layout for UTSA dissertation/thesis. This layout should be used together
%with the UTSAthesis.sty.
%
%
%\subsection{Installation}
%
%First, install UTSAthesis.sty as described in Section \ref{sec:UTSAthesis.sty}.
%Then, installed the \LyX{} on your system by following the instruction
%that comes with the \LyX{} package. Next, place the utsathesis.layout
%into your personal \LyX{} directory. On a Linux/Unix system, this
%directory is at \textasciitilde{}/.lyx/layouts. On Mac OS, it is at
%/User/<name>/Library/Application Support/\LyX{}-<version>/layouts.
%On Windows 7, it is at C:\textbackslash{}Users\textbackslash{}<name>\textbackslash{}AppData\textbackslash{}Roaming\textbackslash{}lyx<version>\textbackslash{}layouts.
%Remember to run Tools->Reconfigure inside \LyX{} to re-configure the
%system.
%
%
%\subsection{Use of utsathesis.layout Package}
%
%This document (sampleThesis.lyx) provides a template for using the
%utsathesis.layout to write a Ph.D. dissertation. For a Master's thesis,
%go to Document->Settings and set the class option to ms. Other important
%settings may include Document->Settings->\LaTeX{} Preamble, and the
%bibliography style.
%
%The document setting should be ``report (UTSAthesis 2012)''. The
%document should begin with committee info, thesis info, copyright,
%and dedication. These can be formatted using items in the FrontMatter
%in the pull-down menu. These should be followed by title, author,
%acknowledgments and the abstract. The placement and the order of these
%four items are important for generating the correctly formatted front
%pages of the thesis/dissertation. It is also important to add the
%``Start First Page'' item right before the first chapter. This item
%will set the correct page numbers for the main portion of the thesis/dissertation.
%
%At the end of the document, the ``Vita'' item in the BackMatter
%in the pull-down menu needs to be used to format a one-page vita.
%
%Regular chapters can be included in the main thesis document or more
%likely as sub-files, one per chapter. If sub-files are preferred,
%make sure the document settings of all sub-files are identical to
%the main document. 
%
%
%\chapter{Literature Review}
%
%We have some citations \cite{dabiri-optimization-isqed-2008,melhem-ieeetc-2003,pradhan-fault-tolerance-1986}.
%See the Bibliography for the format of references.
%
%\include{chapt3}
%
%
%\chapter{Solution and Evaluation}
%
%In this chapter, we show the structures of math formula, theorem commands,
%and floats (such as algorithm and table).
%
%
%\section{A Theory}
%\begin{defn}
%This is another definition.\end{defn}
%\begin{thm}
%This is a theorem.
%\begin{equation}
%X=\frac{AB}{Y}
%\end{equation}
%\end{thm}
%\begin{proof}
%The proof is done here.
%\end{proof}
%
%\section{An Algorithm}
%
%The following is the algorithm.
%
%\begin{algorithm}
%\begin{enumerate}
%\item Step One
%\item Step Two
%\end{enumerate}
%\caption{The Do-It-Yourself Method}
%
%
%\end{algorithm}
%
%
%
%\subsection{Evaluation}
%
%The evaluation results is shown in the following table. It is straightforward
%to place the caption of the table above or below the table.
%
%\begin{table}
%\caption{Evaluation Results}
%
%
%\noindent \centering{}%
%\begin{tabular}{|c|c|c|c|}
%\hline 
% & Method 1 & Method 2 & Method 3\tabularnewline
%\hline 
%\hline 
%Criterion 1 &  &  & \tabularnewline
%\hline 
%Criterion 2 &  &  & \tabularnewline
%\hline 
%Criterion 3 &  &  & \tabularnewline
%\hline 
%\end{tabular}
%\end{table}
%
%
%The following is a long table
%
%\noindent \begin{center}
%\begin{longtable}{|c|c|c|c|c|}
%\caption{A Long Table\label{tab:A-Long-Table}}
%\endfirsthead
%\multicolumn{5}{c}{\textbf{Table \ref{tab:A-Long-Table}}: Continued}\tabularnewline
%\endhead
%\hline 
%Column1 & Column 2 & Column 3 & Column 4 & Column 5\tabularnewline
%\hline 
%\hline 
%1 &  &  &  & \tabularnewline
%\hline 
%2 &  &  &  & \tabularnewline
%\hline 
%3 &  &  &  & \tabularnewline
%\hline 
%4 &  &  &  & \tabularnewline
%\hline 
%5 &  &  &  & \tabularnewline
%\hline 
%6 &  &  &  & \tabularnewline
%\hline 
%7 &  &  &  & \tabularnewline
%\hline 
%8 &  &  &  & \tabularnewline
%\hline 
%9 &  &  &  & \tabularnewline
%\hline 
%10 &  &  &  & \tabularnewline
%\hline 
%11 &  &  &  & \tabularnewline
%\hline 
%12 &  &  &  & \tabularnewline
%\hline 
%13 &  &  &  & \tabularnewline
%\hline 
%14 &  &  &  & \tabularnewline
%\hline 
%15 &  &  &  & \tabularnewline
%\hline 
%16 &  &  &  & \tabularnewline
%\hline 
%17 &  &  &  & \tabularnewline
%\hline 
%18 &  &  &  & \tabularnewline
%\hline 
%19 &  &  &  & \tabularnewline
%\hline 
%20 &  &  &  & \tabularnewline
%\hline 
%21 &  &  &  & \tabularnewline
%\hline 
%22 &  &  &  & \tabularnewline
%\hline 
%23 &  &  &  & \tabularnewline
%\hline 
%24 &  &  &  & \tabularnewline
%\hline 
%25 &  &  &  & \tabularnewline
%\hline 
%26 &  &  &  & \tabularnewline
%\hline 
%27 &  &  &  & \tabularnewline
%\hline 
%28 &  &  &  & \tabularnewline
%\hline 
%29 &  &  &  & \tabularnewline
%\hline 
%30 &  &  &  & \tabularnewline
%\hline 
%31 &  &  &  & \tabularnewline
%\hline 
%32 &  &  &  & \tabularnewline
%\hline 
%33 &  &  &  & \tabularnewline
%\hline 
%\end{longtable}
%\par\end{center}
%
%
%\chapter{Future Directions}
%
%There can be more chapters.
%
%\appendix
%
%\chapter{Notations }
%
%Here we show the use of multiple appendixes.
%
%
%\section{Math Notations}
%
%Each appendix can have sub-sections as a regular chapter.
%
%
%\section{Additional Notations}
%
%
%\chapter{Ontologies}
%
%These is another appendix.

%the individual components. 
