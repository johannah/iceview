{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from shutil import copyfile\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "from multiprocessing import Pool, freeze_support, cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from itertools import islice\n",
    "import cv2\n",
    "from copy import copy, deepcopy\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "import config\n",
    "import math\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "#import utils\n",
    "absolute_min_matches = 10\n",
    "from skimage.io import imread\n",
    "from skimage.color import gray2rgb, rgb2gray\n",
    "from skimage.feature import match_descriptors\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import warp, SimilarityTransform, AffineTransform, ProjectiveTransform\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sift = cv2.SIFT()\n",
    "orb = cv2.ORB()\n",
    "surf = cv2.SURF()\n",
    "brisk = cv2.BRISK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A = cv2.imread('apple.jpg')\n",
    "B = cv2.imread('orange.jpg')\n",
    "    6 \n",
    "    7 # generate Gaussian pyramid for A\n",
    "    8 G = A.copy()\n",
    "    9 gpA = [G]\n",
    "   10 for i in xrange(6):\n",
    "   11     G = cv2.pyrDown(G)\n",
    "   12     gpA.append(G)\n",
    "   13 \n",
    "   14 # generate Gaussian pyramid for B\n",
    "   15 G = B.copy()\n",
    "   16 gpB = [G]\n",
    "   17 for i in xrange(6):\n",
    "   18     G = cv2.pyrDown(G)\n",
    "   19     gpB.append(G)\n",
    "   20 \n",
    "   21 # generate Laplacian Pyramid for A\n",
    "   22 lpA = [gpA[5]]\n",
    "   23 for i in xrange(5,0,-1):\n",
    "   24     GE = cv2.pyrUp(gpA[i])\n",
    "   25     L = cv2.subtract(gpA[i-1],GE)\n",
    "   26     lpA.append(L)\n",
    "   27 \n",
    "   28 # generate Laplacian Pyramid for B\n",
    "   29 lpB = [gpB[5]]\n",
    "   30 for i in xrange(5,0,-1):\n",
    "   31     GE = cv2.pyrUp(gpB[i])\n",
    "   32     L = cv2.subtract(gpB[i-1],GE)\n",
    "   33     lpB.append(L)\n",
    "   34 \n",
    "   35 # Now add left and right halves of images in each level\n",
    "   36 LS = []\n",
    "   37 for la,lb in zip(lpA,lpB):\n",
    "   38     rows,cols,dpt = la.shape\n",
    "   39     ls = np.hstack((la[:,0:cols/2], lb[:,cols/2:]))\n",
    "   40     LS.append(ls)\n",
    "   41 \n",
    "   42 # now reconstruct\n",
    "   43 ls_ = LS[0]\n",
    "   44 for i in xrange(1,6):\n",
    "   45     ls_ = cv2.pyrUp(ls_)\n",
    "   46     ls_ = cv2.add(ls_, LS[i])\n",
    "   47 \n",
    "   48 # image with direct connecting each half\n",
    "   49 real = np.hstack((A[:,:cols/2],B[:,cols/2:]))\n",
    "   50 \n",
    "   51 cv2.imwrite('Pyramid_blending2.jpg',ls_)\n",
    "   52 cv2.imwrite('Direct_blending.jpg',real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_alpha(img, mask=None):\n",
    "    \"\"\"\n",
    "    Adds a masked alpha channel to an image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : (M, N[, 3]) ndarray\n",
    "        Image data, should be rank-2 or rank-3 with RGB channels. If img already has alpha, \n",
    "        nothing will be done. \n",
    "    mask : (M, N[, 3]) ndarray, optional\n",
    "        Mask to be applied. If None, the alpha channel is added\n",
    "        with full opacity assumed (1) at all locations.\n",
    "    \"\"\"\n",
    "    # don't do anything if there is already an alpha channel\n",
    "    #return img\n",
    "    \n",
    "    if img.shape[2] > 3:\n",
    "        return img\n",
    "    # make sure the image is 3 channels\n",
    "    if img.ndim == 2:\n",
    "        img = gray2rgb(img)\n",
    "    if mask is None: \n",
    "        # create transparent mask \n",
    "        # 1 should be fully transparent\n",
    "        mask = np.ones(img.shape[:2], np.uint8)*255\n",
    "    return np.dstack((img, mask))\n",
    "\n",
    "def find_corners(all_corners):\n",
    "    \n",
    "\n",
    "    # The overally output shape will be max - min\n",
    "    corner_min = np.min(all_corners, axis=0)\n",
    "    corner_max = np.max(all_corners, axis=0)\n",
    "    output_shape = (corner_max - corner_min)\n",
    "\n",
    "    # Ensure integer shape with np.ceil and dtype conversion\n",
    "    output_shape = np.ceil(output_shape[::-1]).astype(int)\n",
    "\n",
    "    # This in-plane offset is the only necessary transformation for the base image\n",
    "    offset = SimilarityTransform(translation= -corner_min)\n",
    "    return offset, output_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getKeypointandDescriptors(img, detector):\n",
    "    kps, des = detector.detectAndCompute(img, None)\n",
    "    kp = np.asarray([k.pt for k in kps])\n",
    "    return kp, des\n",
    "\n",
    "def loadImage(img_path, detector):\n",
    "    rgb = add_alpha(cv2.imread(img_path))\n",
    "    img = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "    # Find key points in base image \n",
    "    k, d = getKeypointandDescriptors(img, detector)\n",
    "    return rgb, k, d  \n",
    "\n",
    "\n",
    "def make_chunks(it, size):\n",
    "    return [it[x:x+size] for x in range(0, len(it), size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_matches(matches, ratio = 0.75):\n",
    "    filtered_matches = []\n",
    "    for m in matches:\n",
    "        if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "            filtered_matches.append(m[0])\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def match_from_to(fk, fd, tk, td, matcher):\n",
    "    min_matches = 8\n",
    "    # get matching keypoints between images (from to) or (previous, base) or (next, base)\n",
    "    try:\n",
    "        matches = matcher.knnMatch(fd, td, k=2)\n",
    "        matches_subset = filter_matches(matches)\n",
    "\n",
    "        src = [fk[match.queryIdx] for match in matches_subset]\n",
    "        # target image is base image\n",
    "        dst = [tk[match.trainIdx] for match in matches_subset]\n",
    "\n",
    "        src = np.asarray(src)\n",
    "        dst = np.asarray(dst)\n",
    "\n",
    "        if src.shape[0] > min_matches:\n",
    "            # TODO - select which transform to use based on sensor data?\n",
    "            model_robust, inliers = ransac((src, dst), AffineTransform, min_samples=8, residual_threshold=1)\n",
    "            accuracy = float(inliers.shape[0])/float(src.shape[0])\n",
    "            return model_robust, inliers, accuracy\n",
    "    except Exception, e:\n",
    "        logging.error(e)\n",
    "    return None, None, 0\n",
    "\n",
    "def warp_img(img, transform, output_shape):\n",
    "    try:\n",
    "        warped = warp(img, transform, order=3, mode='constant',\n",
    "                   output_shape=output_shape, cval=0)\n",
    "        return warped\n",
    "    except Exception, e:\n",
    "        logging.error(\"Error warping image %s\" %e)\n",
    "        return None\n",
    "\n",
    "def copy_new_files(input_dir, output_dir, in_ftype, out_ftype, wsize, do_clear, limit):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "        \n",
    "\n",
    "    if do_clear:\n",
    "        \n",
    "        to_clear_mosaics = sorted(glob(os.path.join(output_dir, 'RUN*MATCH*%s'%out_ftype)))\n",
    "        if len(to_clear_mosaics):\n",
    "                logging.warning(\"Clearing RUN files from output_dir: %s\" %output_dir)\n",
    "                for f in to_clear_mosaics:\n",
    "                    os.remove(f)\n",
    "    \n",
    "        #to_clear = sorted(glob(os.path.join(output_dir, '*%s'%out_ftype)))\n",
    "        #if len(to_clear):\n",
    "        #    logging.warning(\"Clearing files from output_dir: %s\" %output_dir)\n",
    "        #    for f in to_clear:\n",
    "        #        os.remove(f)\n",
    "        \n",
    "    logging.info(\"Using convert to transfer and scan input images\")\n",
    "    in_files = sorted(glob(os.path.join(input_dir, '*%s'%in_ftype)))\n",
    "    if limit is not None:\n",
    "        try:\n",
    "            in_files = in_files[:limit]\n",
    "        except:\n",
    "            pass\n",
    "    for iimg in sorted(in_files):\n",
    "        oname = os.path.basename(iimg).split('.')[0] + '.%s' %out_ftype\n",
    "       \n",
    "        ofile = os.path.join(output_dir, oname)\n",
    "        if not os.path.exists(ofile):\n",
    "            cmd = [\"convert\", iimg, \"-resize\", \"%dx%d\" %(wsize[0], wsize[1]), ofile]\n",
    "            subprocess.call(cmd)\n",
    "            #logging.info(\"Calling %s\" %' '.join(cmd))\n",
    "        else:\n",
    "            pass\n",
    "            #logging.debug(\"The file %s already exists\" %ofile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class mosaic():\n",
    "    def __init__(self, inputpath, outpath, input_image_type, do_clear=False, limit=None, wsize=(3000,2000)):\n",
    "        self.detector = cv2.SIFT(4000)\n",
    "        self.outpath = outpath\n",
    "        self.out_ftype = 'png'\n",
    "            # Parameters for nearest-neighbor matching\n",
    "        FLANN_INDEX_KDTREE = 1  # bug: flann enums are missing\n",
    "        flann_params = dict(algorithm = FLANN_INDEX_KDTREE, \n",
    "            trees = 5)\n",
    "        self.matcher = cv2.FlannBasedMatcher(flann_params, {})\n",
    "        self.chunk_size = 3\n",
    "        self.brute_searched = False\n",
    "        copy_new_files(inputpath, outpath, input_image_type, self.out_ftype, wsize, do_clear, limit)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def run_round(self, ROUND_NUM, last_num_imgs=10e6):\n",
    "        img_paths = sorted(glob(os.path.join(self.outpath, '*%s'%self.out_ftype)))\n",
    "        \n",
    "        num_imgs = len(img_paths)\n",
    "        # make sure that we have made some progress\n",
    "        # TODO: if num_imgs doesn't shrink, need to add full search\n",
    "        if num_imgs < last_num_imgs:\n",
    "            self.chunk_size += 3\n",
    "            logging.debug(\"Didn't find any matches last run, increasing search space to: %s\" %self.chunk_size)\n",
    "        logging.info(\"FOUND %s images to stitch in round %s with chunk size: %s\" %(num_imgs, ROUND_NUM, self.chunk_size ))\n",
    "        # time to bring out the big guns\n",
    "        # try to search every other image\n",
    "        # TODO: put a cap on the number of images so this doesnt blow up\n",
    "        if self.chunk_size >= num_imgs:\n",
    "            logger.info(\"Chunk size is larger than number of images - chunk size: %s num_imgs: %s\" %(self.chunk_size, num_imgs))\n",
    "            logger.info(\"brute_forced:%s, num_imgs: %s, last_num_imgs: %s\" %(self.brute_searched, num_imgs, last_num_imgs))\n",
    "\n",
    "            if self.brute_searched and (num_imgs >= last_num_imgs):\n",
    "                logger.error(\"Brute forced true and did not find any matches last run, exiting\")\n",
    "                return\n",
    "            else:\n",
    "                logger.info(\"Entering brute force search\")\n",
    "                for base_index, bn in enumerate(img_paths):\n",
    "                    logger.info(\"Searching bn:%s against all other images\" %os.path.basename(bn))\n",
    "                    self.stitch_chunk(img_paths, base_index, 0, ROUND_NUM)\n",
    "                    ROUND_NUM+=1\n",
    "                    img_paths = sorted(glob(os.path.join(self.outpath, '*%s'%self.out_ftype)))\n",
    "                    logger.info(\"Brute force search against %s found %s matches\" %(os.path.basename(bn), num_imgs-len(img_paths)))\n",
    "                    num_imgs = len(img_paths)\n",
    "                self.brute_searched = True\n",
    "            self.run_round(ROUND_NUM+1, num_imgs) \n",
    "        else:  \n",
    "            # divide into chunks of 3 to match together\n",
    "            chunks = make_chunks(img_paths, self.chunk_size)\n",
    "            # make sure we actually found files\n",
    "            base_index = self.chunk_size/2\n",
    "            if len(chunks) > 1:\n",
    "                for CHUNK_NUM, chunk in enumerate(chunks):\n",
    "                    self.stitch_chunk(chunk, base_index, CHUNK_NUM, ROUND_NUM)\n",
    "                self.run_round(ROUND_NUM+1, num_imgs)   \n",
    "            elif len(chunks) == 1:\n",
    "                logging.info(\"ONLY ONE CHUNK left\")\n",
    "                if len(chunks[0]) == 1:\n",
    "                    logging.info(\"FINISHED!\")\n",
    "                else:\n",
    "                    logging.info(\"Working on last match\")\n",
    "                    self.stitch_chunk(chunks[0], 0, 0, ROUND_NUM)\n",
    "            else:\n",
    "                logging.error(\"DID not find any files\")\n",
    "\n",
    "    def stitch_chunk(self, chunk, base_index, CHUNK_NUM, ROUND_NUM):\n",
    "        # TODO - still off by one eggh\n",
    "        logging.debug(\"WORKING ON CHUNK num: %s of %s ROUND NUM: %s\" %(CHUNK_NUM,' '.join([os.path.basename(c) for c in chunk]),  ROUND_NUM))\n",
    "        if len(chunk) > 1:\n",
    "            # load the center or right image to use as base\n",
    "            bn = chunk.pop(base_index)\n",
    "            brgb, bk, bd = loadImage(bn, self.detector)\n",
    "            \n",
    "            if bk.shape[0] > 8:\n",
    "                # Shape of base image, our registration target\n",
    "                r, c = brgb.shape[:2]\n",
    "\n",
    "                # Note that transformations take coordinates in (x, y) format,\n",
    "                # not (row, column), in order to be consistent with most literature\n",
    "                base_corners = np.array([[0, 0], #image (0,0) coordinate\n",
    "                                    [0, r], # \n",
    "                                    [c, 0],\n",
    "                                    [c, r]])\n",
    "\n",
    "                corners = deepcopy(base_corners)\n",
    "                models = []\n",
    "                match_names = []\n",
    "                iis = []\n",
    "                for name in chunk:\n",
    "                    rgb, k, d = loadImage(name, self.detector)\n",
    "\n",
    "                    model_robust, inliers, accuracy = match_from_to(k, d, bk, bd, self.matcher)\n",
    "                    if accuracy > .2:\n",
    "                        models.append(model_robust)\n",
    "                        iis.append(rgb)\n",
    "                        tcorners = model_robust(base_corners)\n",
    "                        match_names.append(name)\n",
    "                        corners = np.vstack((corners, tcorners))\n",
    "                    else:\n",
    "                        logging.info(\"Not able to match with Base:%s %d, Img: %s %d keypoints\" %(os.path.basename(bn),\n",
    "                                                                                                 bk.shape[0],\n",
    "                                                                                                 os.path.basename(name), \n",
    "                                                                                                 k.shape[0]))\n",
    "                print('NUM MODELS', len(models))\n",
    "\n",
    "\n",
    "                #Used in conjunction with mode ‘constant’, the value outside the image boundaries.\n",
    "                if len(models):\n",
    "                    \n",
    "                    offset, output_shape = find_corners(corners)\n",
    "                    \n",
    "                    brgb_warped = warp_img(brgb, offset.inverse, output_shape)\n",
    "                    if brgb_warped is None:\n",
    "                        logger.error(\"Unable to warp base img %s\" %os.path.basename(bn))\n",
    "                    else:\n",
    "                        tnames = []\n",
    "                        for xxv, (model, i, n) in enumerate(zip(models, iis, match_names)):\n",
    "                            ## Translate base into place\n",
    "                            tname = '/tmp/timg_%02d.png' %xxv\n",
    "                            logging.debug(\"writing tmp %s to match with bn %s as %s\" %(os.path.basename(n), os.path.basename(bn), tname))\n",
    "                            transform = (model + offset).inverse\n",
    "                            rgb_warped = warp_img(i, transform, output_shape)\n",
    "                            if rgb_warped is None:\n",
    "                                logger.error(\"Unable to warp img %s\" %os.path.basename(n))\n",
    "                            else:\n",
    "                                # successful warp\n",
    "                                plt.imsave(tname, rgb_warped)\n",
    "                                tnames.append(tname)\n",
    "                            \n",
    "                                \n",
    "                        if not len(tnames):\n",
    "                            logger.info(\"Not able to match %s images to %s\" %(len(chunk), os.path.basename(n)))\n",
    "                        else:\n",
    "                            bname = '/tmp/bimg.png'\n",
    "                            plt.imsave(bname, brgb_warped)\n",
    "\n",
    "                            tnames.append(bname)\n",
    "                            match_names.append(bn)\n",
    "\n",
    "                            oname = os.path.join(self.outpath, 'RUN%03d_MATCH%03d.%s' %(ROUND_NUM, CHUNK_NUM, self.out_ftype))\n",
    "                            cmd = ['enblend']\n",
    "                            cmd.extend(tnames)\n",
    "                            cmd.extend(['-o', oname])\n",
    "                            logger.info(\"Calling subprocess command: %s\" % ' '.join(cmd))\n",
    "                            subprocess.call(cmd)\n",
    "                            logger.info(\"Wrote %s matches to file: %s\" %(len(tnames), os.path.basename(oname)))\n",
    "                            # TODO - check subprocess call\n",
    "                            # should remove all in tnames\n",
    "                            for f in match_names:\n",
    "                                os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Clearing RUN files from output_dir: /Volumes/johannah_external 1/thesis-work/201511_sea_state_DRI_Sikululiaq/uas_data/seastate_october_20/n2/image/flight_2_out\n",
      "INFO:root:Using convert to transfer and scan input images\n"
     ]
    }
   ],
   "source": [
    "bpath = \"/Volumes/johannah_external 1/thesis-work/201511_sea_state_DRI_Sikululiaq/uas_data/seastate_october_20/n2/image/\"\n",
    "test = False\n",
    "inpath = os.path.join(bpath, \"flight_2\")\n",
    "outpath = os.path.join(bpath, \"flight_2_out\")\n",
    "do_clear = True\n",
    "lsize = (1500, 1000)\n",
    "if test:\n",
    "    inpath = 'jo_patch/'\n",
    "    outpath = 'aout'\n",
    "    do_clear = True\n",
    "    lsize = (400, 200)\n",
    "m = mosaic(inpath, outpath, 'jpg', do_clear=do_clear, limit=100, wsize=lsize)\n",
    "m.run_round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare(*images, **kwargs):\n",
    "    \"\"\"\n",
    "    Utility function to display images side by side.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image0, image1, image2, ... : ndarrray\n",
    "        Images to display.\n",
    "    labels : list\n",
    "        Labels for the different images.\n",
    "    \"\"\"\n",
    "    f, axes = plt.subplots(1, len(images), **kwargs)\n",
    "    axes = np.array(axes, ndmin=1)\n",
    "    \n",
    "    labels = kwargs.pop('labels', None)\n",
    "    if labels is None:\n",
    "        labels = [''] * len(images)\n",
    "    \n",
    "    for n, (image, label) in enumerate(zip(images, labels)):\n",
    "        axes[n].imshow(image, interpolation='nearest', cmap='gray')\n",
    "        axes[n].set_title(label)\n",
    "        axes[n].axis('off')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "6\n",
      "5\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a =[4, 5, 6, 9]\n",
    "for b in range(len(a)):\n",
    "    print(a.pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv::Stitcher is fairly slow. If your cameras definitely don't move relative to one another and the transformation is as simple as you say, you should be able to overlay the images onto a blank canvas simply by chaining homographies.\n",
    "\n",
    "The following is somewhat mathematical - if this isn't clear I can write it up properly using LaTeX, but SO doesn't support pretty maths :)\n",
    "\n",
    "You have a set of 4 cameras, from left to right, (C_1, C_2, C_3, C_4), giving a set of 4 images (I_1, I_2, I_3, I_4).\n",
    "\n",
    "To transform from I_1 to I_2, you have a 3x3 transformation matrix, called a homography. We'll call this H_12. Similarly for I_2 to I_3 we have H_23 and for I_3 to I_4 you'll have H_34.\n",
    "\n",
    "You can pre-calibrate these homographies in advance using the standard method (point matching between the overlapping cameras).\n",
    "\n",
    "You'll need to create a blank matrix, to act as the canvas. You can guess the size of this (4*image_size would suffice) or you can take the top-right corner (call this P1_tr) and transform it by the three homographies, giving a new point at the top-right of the panorama, PP_tr (the following assumes that P1_tr has been converted to a matrix):\n",
    "\n",
    "PP_tr = H_34 * H_23 * H_12 * P1_tr'\n",
    "What this is doing, is taking P1_tr and transforming it first into camera 2, then from C_2 to C_3 and finally from C_3 to C_4\n",
    "\n",
    "You'll need to create one of these for combining images 1 and 2, images 1,2 and 3 and finally images 1-4, I'll refer to them as V_12, V_123 and V_1234 respectively.\n",
    "\n",
    "Use the following to warp the image onto the canvas:\n",
    "\n",
    "cv::warpAffine(I_2, V_12, H_12, V_12.size( ));\n",
    "Then do the same with the next images:\n",
    "\n",
    "cv::warpAffine(I_3, V_123, H_23*H_12, V_123.size( ));\n",
    "cv::warpAffine(I_4, V_1234, H_34*H_23*H_12, V_1234.size( ));\n",
    "Now you have four canvases, all of which are the width of the 4 combined images, and with one of the images transformed into the relevant place on each.\n",
    "\n",
    "All that remains is to merge the transformed images onto eachother. This is easily achieved using regions of interest.\n",
    "\n",
    "Creating the ROI masks can be done in advance, before frame capture begins.\n",
    "\n",
    "Start with a blank (zeros) image the same size as your canvases will be. Set the leftmost rectangle the size of I_1 to white. This is the mask for your first image. We'll call it M_1.\n",
    "\n",
    "Next, to get the mask for the second transformed image, we do\n",
    "\n",
    "cv::warpAffine(M_1, M_2, H_12, M_1.size( ));\n",
    "cv::warpAffine(M_2, M_3, H_23*H_12, M_1.size( ));\n",
    "cv::warpAffine(M_3, M_4, H_34*H_23*H_12, M_1.size( ));\n",
    "To bring all the images together into one panorama, you do:\n",
    "\n",
    "cv::Mat pano = zeros(M_1.size( ), CV_8UC3);\n",
    "I_1.copyTo(pano, M_1);\n",
    "V_12.copyTo(pano, M_2): \n",
    "V_123.copyTo(pano, M_3): \n",
    "V_1234.copyTo(pano, M_4): \n",
    "What you're doing here is copying the relevant area of each canvas onto the output image, pano - a fast operation.\n",
    "\n",
    "You should be able to do all this on the GPU, substituting cv::gpu::Mat's for cv::Mats and cv::gpu::warpAffine for its non-GPU counterpart.\n",
    "\n",
    "shareimprove this answer\n",
    "edited Apr 1 '15 at 21:07\n",
    "answered Apr 1 '15 at 20:54\n",
    "\n",
    "n00dle\n",
    "3,4631835\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from skimage.feature.util import _mask_border_keypoints, DescriptorExtractor\n",
    "class zernike(DescriptorExtractor):\n",
    "    \n",
    "    def __init__(self, descriptor_size=256, patch_size=49,\n",
    "                  sigma=1, sample_seed=1):\n",
    "        self.descriptor_size = descriptor_size\n",
    "        self.patch_size = patch_size\n",
    "        self.sigma = sigma\n",
    "        self.sample_seed = sample_seed\n",
    "\n",
    "        self.descriptors = None\n",
    "        self.mask = None\n",
    "    \n",
    "    def extract(self, image, keypoints):\n",
    "        patch_size = self.patch_size\n",
    "        desc_size = self.descriptor_size\n",
    "        random = np.random.RandomState()\n",
    "        random.seed(self.sample_seed)\n",
    "        samples = (patch_size / 5.0) * random.randn(desc_size * 8)\n",
    "        samples = np.array(samples, dtype=np.int32)\n",
    "        samples = samples[(samples < (patch_size // 2))\n",
    "                          & (samples > - (patch_size - 2) // 2)]\n",
    "\n",
    "        pos1 = samples[:desc_size * 2].reshape(desc_size, 2)\n",
    "        pos2 = samples[desc_size * 2:desc_size * 4].reshape(desc_size, 2)\n",
    "        \n",
    "        pos1 = np.ascontiguousarray(pos1)\n",
    "        pos2 = np.ascontiguousarray(pos2)\n",
    "        self.mask = _mask_border_keypoints(image.shape, keypoints,\n",
    "                                           patch_size // 2)\n",
    "        keypoints = np.array(keypoints[self.mask, :], dtype=np.intp,\n",
    "                             order='C', copy=False)\n",
    "        self.descriptors = np.zeros((keypoints.shape[0], desc_size),\n",
    "                                    dtype=bool, order='C')\n",
    "        \n",
    "        _zern_loop(image, self.descriptors.view(np.uint8), keypoints,\n",
    "                    pos1, pos2)\n",
    "        \n",
    "def _zern_loop(image, descriptors, keypoints, pos0, pos1):\n",
    "    for p in range(pos0.shape[0]):\n",
    "        pr0 = pos0[p, 0]\n",
    "        pc0 = pos0[p, 1]\n",
    "        pr1 = pos1[p, 0]\n",
    "        pc1 = pos1[p, 1]\n",
    "        for k in range(keypoints.shape[0]):\n",
    "            kr = keypoints[k, 0]\n",
    "            kc = keypoints[k, 1]\n",
    "            if image[kr + pr0, kc + pc0] < image[kr + pr1, kc + pc1]:\n",
    "                descriptors[k, p] = True\n",
    "                from mahotas.features import zernike_moments\n",
    "\n",
    "br1 = zernike()\n",
    "#keypoints = corner_peaks(corner_harris(img1), min_distance=5)\n",
    "keypoints1 = corner_peaks(corner_harris(img1, method='eps', eps=.001, sigma=3), min_distance=5)\n",
    "br1.extract(img1, keypoints1)\n",
    "descriptors1 = br1.descriptors\n",
    "keypoints1 = keypoints1[br1.mask]\n",
    "\n",
    "br2 = zernike()\n",
    "#keypoints1 = corner_peaks(corner_harris(img2), min_distance=5)\n",
    "keypoints2 = corner_peaks(corner_harris(img2, method='eps', eps=.001, sigma=3), min_distance=5)\n",
    "br2.extract(img2, keypoints)\n",
    "descriptors2 = br2.descriptors\n",
    "keypoints2 = keypoints2[br2.mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
