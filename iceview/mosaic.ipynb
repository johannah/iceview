{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "#notes:\n",
    "\n",
    "# look at the distribution of pixels to determine which method of keypoints\n",
    "# should be used?\n",
    "\n",
    "# determine neighbors according to the meta-data, match features only among\n",
    "# neighbors\n",
    "\n",
    "# start with smaller scale image, then work up if needed, how to tell which\n",
    "# size is necessary??? - texture changes maybe\n",
    "\n",
    "# should we stack onto each image as we go... or when a new line is started\n",
    "\n",
    "# only select keypoints from the same elevation level\n",
    "\n",
    "# store keypoints, descriptors\n",
    "\n",
    "# what does keypoint, descriptor look like, how do we combine different methods\n",
    "\n",
    "#which is faster, match two, build up\n",
    "#should i be matching subsequent kps, or grabbing keypoints each time?\n",
    "\n",
    "#DO:\n",
    "multi-scale Harris interest point detector\n",
    "download feature scripts\n",
    "invariant to 1-D rotation and scaling\n",
    "Matching is then based on a distance measure between descriptor vectors.\n",
    "Orthogonal moments based on orthogonal polynomials such as Zernike moments have been shown to be invariant to some linear opera- tions, have superior reconstruction capabilities in the presence of noise, and low redundancy compared to other moment representations [57] [27] [29].\n",
    "\n",
    "Our implementation uses the first 25 (n ; 8, m > 0) coefficients in the Zernike expansion of a disk (of radius proportional to the characteristic scale) around all interest points.\n",
    "\n",
    "The set of potential matches is further reduced by examining the ratio of distances between the second best match and the best match. If this is below a given threshold (we use 1.1 since higher values tend to eliminate good matches), the match is considered of low confidence and rejected.\n",
    "\n",
    "Incremental links. This approach initially solves for the global mosaic using the over- laps of the temporal sequence. Notice this is better than simply concatenating pairwise homographies since the transform for each image is determined considering the over- lap to the next and the previous image. Given the initial layout, all new possible overlaps (links in the topology) are verified, this information is incorporated and the transforms for all images are recalculated. This process is repeated until the topology stabilizes and no new links are added. In essence, the global mosaic is created and then refined by adding constraints as new overlaps become apparent.\n",
    "\n",
    "\n",
    " affine transform (linear, 6 parameters) rather than a projective transform is solved for each image, making the matrix to be inverted at each refinement of the topology of size 6N x 6N, with N the number of images.\n",
    " \n",
    " (Top left) Image layout after initial topology estimate (using only temporal overlap information) and proposed links based on possible overlap given the topology esti- mate (top right). (Blue for verified links and green for proposed ones.). Axes correspond to mosaic coordinates (in pixels). Image layout after second (bottom left) and final (bottom right) topology refinements. (Only verified links are shown.)\n",
    " \n",
    " make synthetic survey\n",
    " define goodness parameter\n",
    " decide what method to use for making the mosaic\n",
    " make zernike/multi=scale harris\n",
    " \n",
    "# different resolutions\n",
    "# 204x153\n",
    "# 408x306\n",
    "# 816x612\n",
    "\n",
    "# quality estimates\n",
    "# correlation error\n",
    "\n",
    "# comparison -\n",
    "# autopano, pix4d (3D), ptoptimizer, ptmender\n",
    "# look at spatial relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from skimage.io import ImageCollection, imsave\n",
    "from skimage.color import rgb2hsv, rgb2gray, gray2rgb\n",
    "from skimage.transform import warp, SimilarityTransform, AffineTransform, ProjectiveTransform\n",
    "from skimage.feature import (ORB, match_descriptors, corner_harris, corner_fast, corner_peaks, BRIEF,\n",
    "                             plot_matches)\n",
    "from skimage.measure import ransac\n",
    "from numpy.random import randint\n",
    "from skimage.data import imread\n",
    "from skimage.measure import label\n",
    "\n",
    "from skimage.graph import route_through_array\n",
    "from sklearn.feature_extraction import image\n",
    "from subprocess import Popen, PIPE\n",
    "from multiprocessing import Pool, freeze_support, cpu_count\n",
    "import itertools\n",
    "import math\n",
    "%matplotlib inline\n",
    "path_to_enblend = '/Users/jhansen/Applications/enblend-enfuse-4.0-mac/enblend'\n",
    "tmp_out = '/tmp/out.png'\n",
    "tmp_base = '/tmp/base.png'\n",
    "tmp_img = '/tmp/img.png'\n",
    "data_dir = '../data/jpg/'\n",
    "\n",
    "img1 = imread('IMG_1765.jpg')[:,:,2]\n",
    "img2 = imread('IMG_1767.jpg')[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hsv_imread(img_path):\n",
    "    return rgb2hsv(imread(img_path))\n",
    "\n",
    "def gray_imread(img_path):\n",
    "    return rgb2gray(imread(img_path))\n",
    "\n",
    "def load_images(search_dir, ftype):\n",
    "    search_path = os.path.join(search_dir, '*'+ftype)\n",
    "    files = glob(search_path)\n",
    "    imgs = []\n",
    "    for ff in files:\n",
    "        imgs.append({'name':os.path.split(ff)[1]})\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(*images, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to display images side by side (from skimage example)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image0, image1, ....: ndarray\n",
    "        Images to display\n",
    "    labels: list\n",
    "        Labels for the different images\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(1, len(images), **kwargs)\n",
    "    ax = np.array(ax, ndmin=1)\n",
    "\n",
    "    labels = kwargs.pop('labels', None)\n",
    "    labels = [''] * len(images)\n",
    "    for n, (image, label) in enumerate(zip(images, labels)):\n",
    "        ax[n].imshow(image, interpolation='nearest', cmap=plt.gray())\n",
    "        ax[n].set_title(label)\n",
    "        ax[n].axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_matches(k1, k2, matches):\n",
    "    src = k1[matches[:,0]][:,::-1]\n",
    "    dst = k2[matches[:,1]][:,::-1]\n",
    "    # if there are not enough matches, this fails\n",
    "    model_robust, inliers = ransac((src, dst), AffineTransform,\n",
    "                                   min_samples=20, residual_threshold=1,\n",
    "                                   max_trials=40)\n",
    "\n",
    "    return model_robust, inliers\n",
    "\n",
    "def develop_metadata_mosaic():\n",
    "    # set max pitch and roll angles as qulity\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_two_matches(img1, img2, k1, k2, matches):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    plt.gray()\n",
    "    #plot_matches(ax, img1, img2, k1, k2, matches)\n",
    "    ax[0].imshow(img1)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].scatter(k1[:, 1], k1[:, 0],  facecolors='none', edgecolors='r')\n",
    "\n",
    "    ax[1].imshow(img2)\n",
    "    ax[1].axis('off')\n",
    "    ax[1].scatter(k2[:, 1], k2[:, 0], facecolors='none', edgecolors='r')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_two_keypoints(img1, img2, k1, k2, s1=1, s2=1):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    plt.gray()\n",
    "\n",
    "    ax[0].imshow(img1)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].scatter(k1[:, 1], k1[:, 0], facecolors='none', edgecolors='r')\n",
    "\n",
    "    ax[1].imshow(img2)\n",
    "    ax[1].axis('off')\n",
    "    ax[1].scatter(k2[:, 1], k2[:, 0], facecolors='none', edgecolors='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_costs(diff_image, mask, vertical=True, gradient_cutoff=2.):\n",
    "    \"\"\"\n",
    "    Ensures equal-cost paths from edges to region of interest.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    diff_image : ndarray of floats\n",
    "        Difference of two overlapping images.\n",
    "    mask : ndarray of bools\n",
    "        Mask representing the region of interest in ``diff_image``.\n",
    "    vertical : bool\n",
    "        Control operation orientation.\n",
    "    gradient_cutoff : float\n",
    "        Controls how far out of parallel lines can be to edges before\n",
    "        correction is terminated. The default (2.) is good for most cases.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    costs_arr : ndarray of floats\n",
    "        Adjusted costs array, ready for use.\n",
    "    \"\"\"\n",
    "    if vertical is not True:\n",
    "        return tweak_costs(diff_image.T, mask.T, vertical=vertical,\n",
    "                           gradient_cutoff=gradient_cutoff).T\n",
    "    \n",
    "    # Start with a high-cost array of 1's\n",
    "    costs_arr = np.ones_like(diff_image)\n",
    "    \n",
    "    # Obtain extent of overlap\n",
    "    row, col = mask.nonzero()\n",
    "    cmin = col.min()\n",
    "    cmax = col.max()\n",
    "\n",
    "    # Label discrete regions\n",
    "    cslice = slice(cmin, cmax + 1)\n",
    "    labels = label(mask[:, cslice])\n",
    "    \n",
    "    # Find distance from edge to region\n",
    "    upper = (labels == 0).sum(axis=0)\n",
    "    lower = (labels == 2).sum(axis=0)\n",
    "    \n",
    "    # Reject areas of high change\n",
    "    ugood = np.abs(np.gradient(upper)) < gradient_cutoff\n",
    "    lgood = np.abs(np.gradient(lower)) < gradient_cutoff\n",
    "    \n",
    "    # Give areas slightly farther from edge a cost break\n",
    "    costs_upper = np.ones_like(upper, dtype=np.float64)\n",
    "    costs_lower = np.ones_like(lower, dtype=np.float64)\n",
    "    costs_upper[ugood] = upper.min() / np.maximum(upper[ugood], 1)\n",
    "    costs_lower[lgood] = lower.min() / np.maximum(lower[lgood], 1)\n",
    "    \n",
    "    # Expand from 1d back to 2d\n",
    "    vdist = mask.shape[0]\n",
    "    costs_upper = costs_upper[np.newaxis, :].repeat(vdist, axis=0)\n",
    "    costs_lower = costs_lower[np.newaxis, :].repeat(vdist, axis=0)\n",
    "    \n",
    "    # Place these in output array\n",
    "    costs_arr[:, cslice] = costs_upper * (labels == 0)\n",
    "    costs_arr[:, cslice] +=  costs_lower * (labels == 2)\n",
    "    \n",
    "    # Finally, place the difference image\n",
    "    costs_arr[mask] = diff_image[mask]\n",
    "    \n",
    "    return costs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_enblend(timg, warped):\n",
    "    pass\n",
    "\n",
    "def add_alpha_channel(img, background=-1):\n",
    "    \"\"\"Add an alpha layer to the image.\n",
    "    \n",
    "    The alpha layer is set to 1 for foreground and 0 for background.\n",
    "    \"\"\"\n",
    "    if img.ndim == 2:\n",
    "        img = gray2rgb(img)\n",
    "    return np.dstack((img, (img != background)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_alpha(img, mask=None):\n",
    "    \"\"\"\n",
    "    Adds a masked alpha channel to an image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : (M, N[, 3]) ndarray\n",
    "        Image data, should be rank-2 or rank-3 with RGB channels\n",
    "    mask : (M, N[, 3]) ndarray, optional\n",
    "        Mask to be applied. If None, the alpha channel is added\n",
    "        with full opacity assumed (1) at all locations.\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        mask = np.ones_like(img)\n",
    "        \n",
    "    if img.ndim == 2:\n",
    "        img = gray2rgb(img)\n",
    "    \n",
    "    return np.dstack((img, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_merge(base_warped, img_warped, base_mask, img_mask):\n",
    "    \n",
    "    # Add the three images together. This could create dtype overflows!\n",
    "    # We know they are are floating point images after warping, so it's OK.\n",
    "    merged = (base_warped + img_warped)\n",
    "\n",
    "    # Track the overlap by adding the masks together\n",
    "    # Multiply by 1.0 for bool -> float conversion\n",
    "    overlap = (base_mask * 1.0 + img_mask)\n",
    "\n",
    "    # Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "    norm = merged / np.maximum(overlap, 1)\n",
    "\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_and_extract(detector, img):\n",
    "    detector.detect_and_extract(img)\n",
    "    keypoints = detector.keypoints\n",
    "    descriptors = detector.descriptors\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_output_shape(base_img, model_robust):\n",
    "    r, c = base_img.shape[:2]\n",
    "    \n",
    "    corners = np.array([[0,0], \n",
    "                        [0,r],\n",
    "                        [c,0],\n",
    "                        [c,r]])\n",
    "    \n",
    "    warped_corners = model_robust(corners)\n",
    "    all_corners = np.vstack((warped_corners, corners))\n",
    "    # The overally output shape will be max - min\n",
    "    corner_min = np.min(all_corners, axis=0)\n",
    "    corner_max = np.max(all_corners, axis=0)\n",
    "    output_shape = (corner_max - corner_min)\n",
    "    # Ensure integer shape with np.ceil and dtype conversion\n",
    "    output_shape = np.ceil(output_shape[::-1]).astype(int)\n",
    "    return output_shape, corner_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_two_matches(base_img, img, base_k, img_k, base_d, img_d, min_matches=10):\n",
    "    matches = match_descriptors(base_d, img_d, cross_check=True)\n",
    "    \n",
    "    #   * src (image to be registered): pano2\n",
    "    #   * dst (reference image): pano1, our middle frame registration target\n",
    "    src = img_k[matches[:,1]][:,::-1]\n",
    "    dst = base_k[matches[:,0]][:,::-1]\n",
    "    \n",
    "    # if there are not enough matches, this fails\n",
    "    if matches.shape[0] > min_matches:\n",
    "        model_robust, inliers = ransac((src, dst), ProjectiveTransform,\n",
    "                                   min_samples=8, residual_threshold=1,\n",
    "                                   max_trials=600)\n",
    "\n",
    "        ransac_matches = matches[inliers]\n",
    "        return model_robust, ransac_matches\n",
    "    else:\n",
    "        return np.zeros((0, 2)), np.zeros((0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_keypoints = 800\n",
    "pano_imgs = ImageCollection('*.jpg')\n",
    "img_col = load_images('../data/jpg/', 'jpg')\n",
    "img_feat = {}\n",
    "num_imgs = len(img_col)\n",
    "min_matches = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_empty_edges(img):\n",
    "    def get_mask(sums):\n",
    "        if sum(sums) > 0:\n",
    "            first = sums.index(1)\n",
    "            last = sums[::-1].index(1)\n",
    "\n",
    "            num_ones = (len(sums)-first)-last\n",
    "            out = [0]*first + [1]*num_ones + [0]*last\n",
    "            return out\n",
    "        else:\n",
    "            return sums\n",
    "    \n",
    "    #for ax in range(len(img.shape)):\n",
    "    axes = [0, 1]\n",
    "    for ax in range(2):\n",
    "        sums = np.sum(img, axis=axes[ax])\n",
    "        # make a mask of zero lines in image \n",
    "        sums= [bool(x) for x in sums]\n",
    "        empty = get_mask(list(sums)) \n",
    "        img = np.compress(empty, img, axis=axes[ax-1])\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Read SIFT and SURF feature files.\n",
    "See Also\n",
    "--------\n",
    "http://people.cs.ubc.ca/~lowe/keypoints/\n",
    "http://www.vision.ee.ethz.ch/~surf/\n",
    "\"\"\"\n",
    "\n",
    "__all__ = ['load_sift', 'load_surf']\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _sift_read(f, mode='SIFT'):\n",
    "    \"\"\"Read SIFT or SURF features from a file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : string or open file\n",
    "        Input file generated by the feature detectors from\n",
    "        http://people.cs.ubc.ca/~lowe/keypoints/ or\n",
    "        http://www.vision.ee.ethz.ch/~surf/\n",
    "    Returns\n",
    "    -------\n",
    "    data : record array with fields\n",
    "      - row: int\n",
    "          row position of feature\n",
    "      - column: int\n",
    "          column position of feature\n",
    "      - scale: float\n",
    "          feature scale\n",
    "      - orientation: float\n",
    "          feature orientation\n",
    "      - data: array\n",
    "          feature values\n",
    "    \"\"\"\n",
    "    if not hasattr(f, 'readline'):\n",
    "        f = file(f, 'r')\n",
    "\n",
    "    if mode == 'SIFT':\n",
    "        nr_features, feature_len = map(int, f.readline().split())\n",
    "        datatype = np.dtype([('row', float), ('column', float),\n",
    "                             ('scale', float), ('orientation', float),\n",
    "                             ('data', (float, feature_len))])\n",
    "    else:\n",
    "        mode = 'SURF'\n",
    "        feature_len = int(f.readline()) - 1\n",
    "        nr_features = int(f.readline())\n",
    "        datatype = np.dtype([('column', float), ('row', float),\n",
    "                             ('second_moment', (float, 3)),\n",
    "                             ('sign', float), ('data', (float, feature_len))])\n",
    "    data = np.fromfile(f, sep=' ')\n",
    "    if data.size != nr_features * datatype.itemsize / np.dtype(float).itemsize:\n",
    "        raise IOError(\"Invalid %s feature file.\" % mode)\n",
    "\n",
    "    return data.view(datatype)\n",
    "\n",
    "\n",
    "def load_sift(f):\n",
    "    return _sift_read(f, mode='SIFT')\n",
    "\n",
    "\n",
    "def load_surf(f):\n",
    "    return _sift_read(f, mode='SURF')\n",
    "\n",
    "load_sift.__doc__ = _sift_read.__doc__\n",
    "load_surf.__doc__ = _sift_read.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named surf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-07d3476d8434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msurf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named surf"
     ]
    }
   ],
   "source": [
    "import surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'surf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-da7cf39dcf6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# not working - descriptor is not correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurf_detect_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurf_detect_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplot_two_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-da7cf39dcf6d>\u001b[0m in \u001b[0;36msurf_detect_and_extract\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msurf_detect_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#points = surf.interest_points(img, 6, 24, 1, max_points=1024)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#descs = surf.descriptors(img, points, descriptor_only=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'surf' is not defined"
     ]
    }
   ],
   "source": [
    "def surf_detect_and_extract(img):\n",
    "    ip = surf.surf(img)\n",
    "    #points = surf.interest_points(img, 6, 24, 1, max_points=1024)\n",
    "    #descs = surf.descriptors(img, points, descriptor_only=True)\n",
    "    print(ip.shape)\n",
    "    k = ip[:, :2]\n",
    "    d = ip[:,6:]\n",
    "    def rotate(y,x, a):\n",
    "        sa = np.sin(a)\n",
    "        ca = np.cos(a)\n",
    "        return (ca*x-sa*y, sa*x+ca*y)\n",
    "    return k, d, ip\n",
    "# not working - descriptor is not correct\n",
    "k1, d1, ip1 = surf_detect_and_extract(img1)\n",
    "k2, d2, ip2 = surf_detect_and_extract(img2)\n",
    "plot_two_keypoints(img1, img2, k1, k2)\n",
    "f1 = surf.show_surf(img1, ip1)\n",
    "f2 = surf.show_surf(img2, ip2)\n",
    "plt.figure()\n",
    "plt.imshow(f1)\n",
    "plt.figure()\n",
    "plt.imshow(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import CENSURE\n",
    "from mahotas.features import zernike_moments\n",
    "\n",
    "from mahotas.features import surf\n",
    "import timeit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-675ce6e32b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'took'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-675ce6e32b84>\u001b[0m in \u001b[0;36mz\u001b[0;34m(img1, img2)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_descriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     model_robust, ransac_matches = find_two_matches(img1, img2, \n\u001b[1;32m     19\u001b[0m                                                     \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jhansen/anaconda/envs/py2icevview/lib/python2.7/site-packages/skimage/feature/match.pyc\u001b[0m in \u001b[0;36mmatch_descriptors\u001b[0;34m(descriptors1, descriptors2, metric, p, max_distance, cross_check)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mindices1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptors1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mindices2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcross_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jhansen/anaconda/envs/py2icevview/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmin\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "import skimage.data as data\n",
    "def z(img1, img2):\n",
    "    br = zernike()\n",
    "    #k1 = corner_peaks(corner_harris(img1, method='eps', eps=.001, sigma=3), min_distance=5)\n",
    "    k1 = corner_peaks(corner_fast(img1, n=4, threshold=.001), min_distance=5)\n",
    "    br.extract(img1, k1)\n",
    "    d1 = br.descriptors\n",
    "    k1 = k1[br.mask]\n",
    "\n",
    "    #k2 = corner_peaks(corner_harris(img2, method='eps', eps=.001, sigma=3), min_distance=5)\n",
    "    k2 = corner_fast(img2, n=4, threshold=.001)\n",
    "    br = zernike()\n",
    "    br.extract(img2, k2)\n",
    "    d2 = br.descriptors\n",
    "    k2 = k2[br.mask]\n",
    "\n",
    "    matches = match_descriptors(d1, d2, cross_check=True)\n",
    "    model_robust, ransac_matches = find_two_matches(img1, img2, \n",
    "                                                    k1, k2, \n",
    "                                                    d1, d2)\n",
    "    #fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    #plt.gray()\n",
    "    #plot_matches(ax, img1, img2, k1, k2, ransac_matches)\n",
    "    prec = round(ransac_matches.shape[0]/float(matches.shape[0]), 2)\n",
    "\n",
    "    #print('zern keypoints', k1.shape[0], k2.shape[0], 'matches', matches.shape[0],ransac_matches.shape[0], prec )\n",
    "    return prec\n",
    "def o(img1, img2):\n",
    "    # fast_threshold - decide whether pixels are brighter or darker,\n",
    "    # decrease for more corners\n",
    "    # harris_k smaller for detection of sharp corners\n",
    "    orb = ORB(n_keypoints=600, fast_n=5, \n",
    "              fast_threshold=0.02, \n",
    "              harris_k=.01, n_scales=10)\n",
    "    # Zernike moments are not a texture feature, but rather a global measure of how the mass is distributed.\n",
    "\n",
    "    #k1, d1 = detect_and_extract(orb, img1)\n",
    "    #k2, d2 = detect_and_extract(orb, img2)\n",
    "\n",
    "    #matches = match_descriptors(d1, d2, cross_check=True)\n",
    "    #model_robust, ransac_matches = find_two_matches(img1, img2, \n",
    "    #                                                k1, k2, \n",
    "    #                                                d1, d2)\n",
    "    #prec = round(ransac_matches.shape[0]/float(matches.shape[0]), 2)\n",
    "    #print('orb keypoints', k1.shape[0], k2.shape[0], 'matches', matches.shape[0],  ransac_matches.shape[0], prec )\n",
    "    #fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    #plt.gray()\n",
    "    #plot_matches(ax, img1, img2, k1, k2, ransac_matches)\n",
    "    #return prec\n",
    "a = 10\n",
    "st = time.time()\n",
    "for i in range(a):\n",
    "    z(img1, img2)\n",
    "print('took', (time.time()-st)/float(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((122, 2), (103, 2), (103, 256))\n"
     ]
    }
   ],
   "source": [
    "def z(img1):\n",
    "    br = zernike()\n",
    "    #k1 = corner_peaks(corner_harris(img1, method='eps', eps=.001, sigma=3), min_distance=5)\n",
    "    k1 = corner_peaks(corner_fast(img1, n=4, threshold=.001), min_distance=5)\n",
    "    br.extract(img1, k1)\n",
    "    d1 = br.descriptors\n",
    "    kk = k1[br.mask]\n",
    "    print(k1.shape, kk.shape, d1.shape)\n",
    "z(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[117, 117, 118, ..., 119, 120, 120],\n",
       "       [118, 118, 118, ..., 117, 120, 119],\n",
       "       [118, 118, 118, ..., 118, 119, 119],\n",
       "       ..., \n",
       "       [113, 114, 114, ..., 119, 118, 118],\n",
       "       [113, 114, 115, ..., 119, 118, 117],\n",
       "       [114, 114, 115, ..., 118, 117, 117]], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rotate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-021ec6669d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffineTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimg5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rotate' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "tform = AffineTransform(scale=(1.2,1.2),translation=(0,-100))\n",
    "img3 = warp(img2, tform)\n",
    "img4 = rotate(img2, 25)\n",
    "img5 = rotate(img3, 25)\n",
    "\n",
    "poh, pohw, pohr, pohwr = 0,0,0,0\n",
    "zoh, zohw, zohr, zohwr = 0,0,0,0\n",
    "a = 1\n",
    "for x in range(a):\n",
    "    poh += o(img1, img2)\n",
    "    pohw += o(img1, img3)\n",
    "    pohr += o(img1, img4)\n",
    "    pohwr += o(img1, img5)\n",
    "for x in range(a):\n",
    "    zoh += z(img1, img2)\n",
    "    zohw += z(img1, img3)\n",
    "    zohr += z(img1, img4)\n",
    "    zohwr += z(img1, img5)\n",
    "\n",
    "oa = [poh/float(a), pohw/float(a), pohr/float(a), pohwr/float(a)]\n",
    "za = [zoh/float(a), zohw/float(a), zohr/float(a), zohwr/float(a)]\n",
    "\n",
    "#img1 = imread('IMG_1765.jpg')[:,:,2]\n",
    "#img2 = imread('IMG_1767.jpg')[:,:,2]\n",
    "img1 = imread('IMG_1771.jpg')[:,:,2]\n",
    "img2 = imread('IMG_1772.jpg')[:,:,2]\n",
    "tform = AffineTransform(scale=(1.2,1.2),translation=(0,-100))\n",
    "img3 = warp(img2, tform)\n",
    "img4 = rotate(img2, 25)\n",
    "img5 = rotate(img3, 25)\n",
    "\n",
    "poh, pohw, pohr, pohwr = 0,0,0,0\n",
    "zoh, zohw, zohr, zohwr = 0,0,0,0\n",
    "\n",
    "for x in range(a):\n",
    "    poh += o(img1, img2)\n",
    "    pohw += o(img1, img3)\n",
    "    pohr += o(img1, img4)\n",
    "    pohwr += o(img1, img5)\n",
    "for x in range(a):\n",
    "    zoh += z(img1, img2)\n",
    "    zohw += z(img1, img3)\n",
    "    zohr += z(img1, img4)\n",
    "    zohwr += z(img1, img5)\n",
    "\n",
    "eoa = [poh/float(a), pohw/float(a), pohr/float(a), pohwr/float(a)]\n",
    "eza = [zoh/float(a), zohw/float(a), zohr/float(a), zohwr/float(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ORB Easy Images\t63.2\t51.3\t0.567\t37.3\n",
    "#Zernike Easy Images\t69.6\t59.5\t10.6\t15.6\n",
    "#ORB Hard Images\t42.5\t30.8\t47.2\t31.4\n",
    "#Zernike Hard Images\t57.3\t37.5\t7.5\t14.3\n",
    "import pandas as pd\n",
    "rows=['ORB Easy Images', 'Zernike Easy Images', 'ORB Hard Images', 'Zernike Hard Images', ]\n",
    "cols= [\"Sample Image\", \"Scaled\", \"Rotated\", \"Scaled and Rotated\"]\n",
    "d = np.array([oa, za, eoa, eza])\n",
    "p = pd.DataFrame(d, index=rows, columns=cols)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p.T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _zern_loop(image, descriptors, keypoints, pos0, pos1):\n",
    "    for p in range(pos0.shape[0]):\n",
    "        pr0 = pos0[p, 0]\n",
    "        pc0 = pos0[p, 1]\n",
    "        pr1 = pos1[p, 0]\n",
    "        pc1 = pos1[p, 1]\n",
    "        for k in range(keypoints.shape[0]):\n",
    "            kr = keypoints[k, 0]\n",
    "            kc = keypoints[k, 1]\n",
    "            if image[kr + pr0, kc + pc0] < image[kr + pr1, kc + pc1]:\n",
    "                descriptors[k, p] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.feature.util import _mask_border_keypoints, DescriptorExtractor\n",
    "class zernike(DescriptorExtractor):\n",
    "    \n",
    "    def __init__(self, descriptor_size=256, patch_size=49,\n",
    "                  sigma=1, sample_seed=1):\n",
    "        self.descriptor_size = descriptor_size\n",
    "        self.patch_size = patch_size\n",
    "        self.sigma = sigma\n",
    "        self.sample_seed = sample_seed\n",
    "\n",
    "        self.descriptors = None\n",
    "        self.mask = None\n",
    "    \n",
    "    def extract(self, image, keypoints):\n",
    "        patch_size = self.patch_size\n",
    "        desc_size = self.descriptor_size\n",
    "        random = np.random.RandomState()\n",
    "        random.seed(self.sample_seed)\n",
    "        samples = (patch_size / 5.0) * random.randn(desc_size * 8)\n",
    "        samples = np.array(samples, dtype=np.int32)\n",
    "        samples = samples[(samples < (patch_size // 2))\n",
    "                          & (samples > - (patch_size - 2) // 2)]\n",
    "\n",
    "        pos1 = samples[:desc_size * 2].reshape(desc_size, 2)\n",
    "        pos2 = samples[desc_size * 2:desc_size * 4].reshape(desc_size, 2)\n",
    "        \n",
    "        pos1 = np.ascontiguousarray(pos1)\n",
    "        pos2 = np.ascontiguousarray(pos2)\n",
    "        self.mask = _mask_border_keypoints(image.shape, keypoints,\n",
    "                                           patch_size // 2)\n",
    "        keypoints = np.array(keypoints[self.mask, :], dtype=np.intp,\n",
    "                             order='C', copy=False)\n",
    "        self.descriptors = np.zeros((keypoints.shape[0], desc_size),\n",
    "                                    dtype=bool, order='C')\n",
    "        \n",
    "        _zern_loop(image, self.descriptors.view(np.uint8), keypoints,\n",
    "                    pos1, pos2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One caveat to look out for when utilizing Zernike moments for shape description \n",
    "is the scaling and translation of the object in the image. Depending on where \n",
    "the image is translated in the image, your Zernike moments will be drastically different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_all_matches(unmatched, \n",
    "                     matched=[], \n",
    "                     fail_limit=3,\n",
    "                     num_keypoints=800,\n",
    "                     min_to_match=20, \n",
    "                     do_plot=False):\n",
    "    \n",
    "    num_unmatched = len(unmatched)\n",
    "    if num_unmatched == 0:\n",
    "        print(\"NONE left unmatched\")\n",
    "        return matched\n",
    "    if num_unmatched == 1:\n",
    "        print(\"ONE left unmatched\")\n",
    "        matched.append(unmatched[0])\n",
    "        return matched\n",
    "    \n",
    "    print(\"=====================================\")\n",
    "    base = unmatched[0]\n",
    "    if 'img' in base.keys():\n",
    "        base_img = base['img']\n",
    "    else:\n",
    "        base_img = imread(os.path.join(data_dir, base['name']))\n",
    "        \n",
    "    orb = ORB(n_keypoints=num_keypoints, fast_threshold=0.05)\n",
    "\n",
    "    base_unmatched = []\n",
    "    \n",
    "    \n",
    "    base_name = base['name'].split('.')[0]\n",
    "    base_matched = 0\n",
    "\n",
    "    # go through each image that is not yet matched\n",
    "    for xx, timg in enumerate(unmatched[1:]):\n",
    "        # if the image has not yet been loaded, load it now\n",
    "        if 'img' not in timg.keys():\n",
    "            timg['img'] = imread(os.path.join(data_dir, timg['name']))\n",
    "            \n",
    "        # for now, only use the 3rd channel\n",
    "        img = timg['img'][:,:,2]\n",
    "        \n",
    "        \n",
    "        base_k, base_d = detect_and_extract(orb, base_img[:,:,2])\n",
    "        # if we haven't recorded the keypoints for this image, get them now\n",
    "        if 'keypoints' in timg.keys():\n",
    "            img_k = timg['keypoints']\n",
    "            img_d = timg['descriptors']\n",
    "        else:\n",
    "            img_k, img_d = detect_and_extract(orb, img)\n",
    "            timg['keypoints'] = img_k\n",
    "            timg['descriptors'] = img_d\n",
    "\n",
    "        \n",
    "        matches = match_descriptors(base_d, img_d, cross_check=True)\n",
    "        \n",
    "        model_robust, ransac_matches = find_two_matches(base_img[:,:,2], img, \n",
    "                                                            base_k, img_k, \n",
    "                                                            base_d, img_d)\n",
    "        if do_plot:\n",
    "            print('matches', matches.shape[0], ransac_matches.shape[0])\n",
    "            #plot_two_keypoints(ax, base_img, img, base_k, img_k)\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,12))\n",
    "            plt.title('%s #### %s' %(base_name, timg['name']))\n",
    "            plt.gray()\n",
    "\n",
    "            ax[0].imshow(base_img)\n",
    "            ax[0].axis('off')\n",
    "            ax[0].scatter(base_k[:, 1], base_k[:, 0], facecolors='none', edgecolors='r')\n",
    "\n",
    "            ax[1].imshow(img)\n",
    "            ax[1].axis('off')\n",
    "            ax[1].scatter(img_k[:, 1], img_k[:, 0], facecolors='none', edgecolors='r')\n",
    "            plt.show()\n",
    "        \n",
    "        if ransac_matches.shape[0] < min_to_match:\n",
    "            print(\"------------\", matches.shape[0], \"ransac\", ransac_matches.shape[0])\n",
    "            base_unmatched.append(timg)\n",
    "            if len(base_unmatched) >= fail_limit:\n",
    "                # add two since we've already added this timg\n",
    "                base_unmatched.extend(unmatched[xx+2:])\n",
    "                break\n",
    "        else:\n",
    "            #print('ransac matches', ransac_matches.shape)\n",
    "            base_img = find_mask(base_img, timg['img'], model_robust) \n",
    "            base_img = img\n",
    "            base_matched += 1\n",
    "            print(\"***********\", base_matched)\n",
    "            base_name+= '_' + timg['name'].split('.')[0]\n",
    "            # AFTER an image has been matched, remove from memory\n",
    "            \n",
    "            \n",
    "\n",
    "    # if we were able to match some images to this base_img that\n",
    "    # were not matched in the last run, call again until \n",
    "    # the number of unmatched images stops decreasing\n",
    "    \n",
    "    #print('num previous unmatched', num_unmatched)\n",
    "    #print(\"could not match %s out of %s imgs\" %(len(base_unmatched), \n",
    "    #                                            len(unmatched)-1))\n",
    "    # not_matched must be > 0\n",
    "    # the new number of matches must be less than last time's not matched\n",
    "    \n",
    "    \n",
    "    if 'base_matched' in base.keys():\n",
    "        all_base_matched = base['base_matched'] + base_matched\n",
    "    else:\n",
    "        all_base_matched = 1 + base_matched\n",
    "    rr = {'name':base_name, 'img':base_img, 'base_matched':base_matched}\n",
    "    if (len(base_unmatched)) > 0:\n",
    "        #if base_matched > 0 :\n",
    "            #base_unmatched.insert(0, rr)\n",
    "            #print(\"!!!!!!!!!!! 1 match %s, unmatch %s\" %(len(matched), len(base_unmatched)))\n",
    "            \n",
    "            #return find_all_matches(base_unmatched, matched, num_keypoints, min_to_match)\n",
    "        #else:\n",
    "        print(\"DECLARING %s as matched, ending with %s matches\" %(base_name, base_matched))\n",
    "        matched.append(rr)\n",
    "        #print(\"!!!!!!!!!!! 2 match %s, unmatch %s\" %(len(matched), len(base_unmatched)))\n",
    "        return find_all_matches(base_unmatched, matched, fail_limit, num_keypoints, min_to_match, do_plot)\n",
    "    else:\n",
    "        matched.append(rr)\n",
    "        #print(\"!!!!!!!!!!! 3 match %s, unmatch %s\" %(len(matched), len(base_unmatched)))\n",
    "        return matched\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimum_cost_merge(base_warped, img_warped, base_mask, img_mask):\n",
    "    # Start with the absolute value of the difference image.\n",
    "    # np.abs is necessary because we don't want negative costs!\n",
    "    costs = generate_costs(np.abs(img_warped - base_warped),\n",
    "                           img_mask & base_mask)\n",
    "    costs[0,  :] = 0\n",
    "    costs[-1, :] = 0\n",
    "\n",
    "    output_shape = base_warped.shape\n",
    "    # Arguments are:\n",
    "    #   cost array\n",
    "    #   start pt\n",
    "    #   end pt\n",
    "    #   can it traverse diagonally\n",
    "    ymax = output_shape[1] - 1\n",
    "    xmax = output_shape[0] - 1\n",
    "\n",
    "    # Start anywhere along the top and bottom, left of center.\n",
    "    mask_pts01 = [[0,    ymax // 3],\n",
    "                  [xmax, ymax // 3]]\n",
    "\n",
    "    # Start anywhere along the top and bottom, right of center.\n",
    "    mask_pts12 = [[0,    2*ymax // 3],\n",
    "                  [xmax, 2*ymax // 3]]\n",
    "    \n",
    "    pts, _ = route_through_array(costs, mask_pts01[0], mask_pts01[1], fully_connected=True)\n",
    "\n",
    "    # Convert list of lists to 2d coordinate array for easier indexing\n",
    "    pts = np.array(pts)\n",
    "   \n",
    "    # Start with an array of zeros and place the path\n",
    "    _img_mask = np.zeros_like(img_warped, dtype=np.uint8)\n",
    "    _img_mask[pts[:, 0], pts[:, 1]] = 1\n",
    "\n",
    "\n",
    "    # Labeling starts with zero at point (0, 0)\n",
    "    _img_mask[label(_img_mask, connectivity=1) == 0] = 1\n",
    "    \n",
    "    _base_mask = ~(_img_mask).astype(bool)\n",
    "\n",
    "    base_color = gray2rgb(base_warped)\n",
    "    img_color = gray2rgb(img_warped)\n",
    "    base_final = add_alpha(base_warped, _base_mask)\n",
    "    img_final = add_alpha(img_warped, _img_mask)\n",
    "    \n",
    "    # Start with empty image\n",
    "    base_combined = np.zeros_like(base_warped)\n",
    "    \n",
    "    base_combined += base_warped * _base_mask\n",
    "    base_combined += img_warped * _img_mask\n",
    "    \n",
    "    return base_combined\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(0, 100, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def patchmaker(img, imsize=(255,255), percent_overlap=10):\n",
    "    \"\"\"\n",
    "    Split an image into overlapping patches\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : ndarray\n",
    "        Image from which to extract patches\n",
    "    imsize : tuple of ints\n",
    "        size of patches\n",
    "    percent_overlap : int\n",
    "        Percent as int of overlap desired between overlapping images\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    patches : list of imsize overlapping segments of the image\n",
    "        \n",
    "    \"\"\"\n",
    "    # store the patches here\n",
    "    patches = []\n",
    "    patch_rows = imsize[0]\n",
    "    patch_cols = imsize[1]\n",
    "    if 0 < percent_overlap < 100:\n",
    "        # determine how many pixels to overlap\n",
    "        non_overlap_rows = int(patch_rows*.01*(100-percent_overlap))\n",
    "        non_overlap_cols = int(patch_cols*.01*(100-percent_overlap))\n",
    "    else:\n",
    "        non_overlap_rows = patch_rows\n",
    "        non_overlap_cols = patch_cols\n",
    "        \n",
    "    # row indexes into the original image \n",
    "    r1, c1 = 0,0\n",
    "    # column indexes into the original image\n",
    "    r2, c2 = imsize\n",
    "    # while the last index of the patch image is less than the size of the original, keep going\n",
    "    while r2 < img.shape[0]:\n",
    "        c1 = 0\n",
    "        c2 = c1 + patch_cols\n",
    "        while c2 < img.shape[1]:\n",
    "            patch = img[r1:r2, c1:c2]\n",
    "            patches.append(patch)\n",
    "            c1 += non_overlap_rows\n",
    "            c2 = c1 + patch_cols \n",
    "        r1 += non_overlap_rows\n",
    "        r2 = r1 + patch_rows\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_mask(base_img, img, model_robust):\n",
    "    # what type of interpolation\n",
    "    # 0: nearest-neighbor\n",
    "    # 1: bi-linear\n",
    "    warp_order = 1\n",
    "\n",
    "    output_shape, corner_min = find_output_shape(base_img, model_robust)\n",
    "    #print(\"output_shape\", output_shape, corner_min)\n",
    "    #print(model_robust.scale, model_robust.translation, model_robust.rotation)\n",
    "    \n",
    "    # This in-plane offset is the only necessary transformation for the base image\n",
    "    offset = SimilarityTransform(translation= -corner_min)\n",
    "    base_warped = warp(base_img[:,:,2], offset.inverse, order=warp_order, \n",
    "                      output_shape = output_shape, cval=-1)\n",
    "    base_color = warp(base_img, offset.inverse, order=warp_order, \n",
    "                      output_shape = output_shape, cval=-1)   \n",
    "    # warp image corners to new position in mosaic\n",
    "    transform = (model_robust + offset).inverse\n",
    "    \n",
    "    img_warped = warp(img[:,:,2], transform, order=warp_order, \n",
    "                      output_shape=output_shape, cval=-1)\n",
    "    img_color = warp(img, transform, order=warp_order, \n",
    "                      output_shape=output_shape, cval=-1)\n",
    "    base_mask = (base_warped != -1)\n",
    "    base_warped[~base_mask] = 0\n",
    "\n",
    "    img_mask = (img_warped != -1)\n",
    "    img_warped[~img_mask] = 0\n",
    "\n",
    "    #convert to rgb\n",
    "    #base_alpha = add_alpha(base_color, base_mask)\n",
    "    img_alpha = np.dstack((img_color, img_mask))\n",
    "    base_alpha = np.dstack((base_color, base_mask))\n",
    "\n",
    "    plt.imsave(tmp_base, base_alpha )\n",
    "    plt.imsave(tmp_img, img_alpha )\n",
    "    cmd = [path_to_enblend, tmp_base, tmp_img, '-o', tmp_out]\n",
    "\n",
    "    p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    output, err = p.communicate(b\"input data that is passed to subprocess' stdin\")\n",
    "    rc = p.returncode\n",
    "    # remove alpha channel\n",
    "    \n",
    "    if os.path.exists(tmp_out):\n",
    "        out = imread(tmp_out)[:,:,:3]\n",
    "    else:\n",
    "        print(\"couldnt find out image\")\n",
    "        print(rc, output, err)\n",
    "        plt.figure()\n",
    "        plt.imshow(base_alpha)\n",
    "        plt.figure()\n",
    "\n",
    "        plt.imshow(img_alpha)\n",
    "        plt.show()\n",
    "        out = base_alpha[:,:,:3]\n",
    "    #if you don't have enblend, you can use one of these\n",
    "    #merged_img = simple_merge(base_warped, img_warped, base_mask, img_mask)\n",
    "    #merged_img = minimum_cost_merge(base_warped, img_warped, base_mask, img_mask)\n",
    "    #merged_edges = remove_empty_edges(merged_img)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_alpha(base_img, img, model_robust):\n",
    "    # what type of interpolation\n",
    "    # 0: nearest-neighbor\n",
    "    # 1: bi-linear\n",
    "    warp_order = 1\n",
    "\n",
    "    output_shape, corner_min = find_output_shape(base_img, model_robust)\n",
    "    #print(\"output_shape\", output_shape, corner_min)\n",
    "    #print(model_robust.scale, model_robust.translation, model_robust.rotation)\n",
    "    \n",
    "    # This in-plane offset is the only necessary transformation for the base image\n",
    "    offset = SimilarityTransform(translation= -corner_min)\n",
    "    base_warped = warp(base_img[:,:,2], offset.inverse, order=warp_order, \n",
    "                      output_shape = output_shape, cval=-1)\n",
    "    base_color = warp(base_img, offset.inverse, order=warp_order, \n",
    "                      output_shape = output_shape, cval=-1)   \n",
    "    # warp image corners to new position in mosaic\n",
    "    transform = (model_robust + offset).inverse\n",
    "    \n",
    "    #img_warped = warp(img[:,:,2], transform, order=warp_order, \n",
    "    #                  output_shape=output_shape, cval=-1)\n",
    "    img_color = warp(img, transform, order=warp_order, \n",
    "                      output_shape=output_shape, cval=-1)\n",
    "    #base_mask = (base_warped != -1)\n",
    "    #base_warped[~base_mask] = 0\n",
    "\n",
    "    img_mask = (img_warped != -1)\n",
    "    #img_warped[~img_mask] = 0\n",
    "\n",
    "    #convert to rgb\n",
    "    #base_alpha = add_alpha(base_color, base_mask)\n",
    "    img_alpha = np.dstack((img_color, img_mask))\n",
    "    #base_alpha = np.dstack((base_color, base_mask))\n",
    "\n",
    "    #plt.imsave(tmp_base, base_alpha )\n",
    "    #plt.imsave(tmp_img, img_alpha )\n",
    "    #cmd = [path_to_enblend, tmp_base, tmp_img, '-o', tmp_out]\n",
    "\n",
    "    #p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    #output, err = p.communicate(b\"input data that is passed to subprocess' stdin\")\n",
    "    #rc = p.returncode\n",
    "    # remove alpha channel\n",
    "    \n",
    "    #if os.path.exists(tmp_out):\n",
    "    #    out = imread(tmp_out)[:,:,:3]\n",
    "    #else:\n",
    "    #    print(\"couldnt find out image\")\n",
    "    #    print(rc, output, err)\n",
    "    #    plt.figure()\n",
    "    #    plt.imshow(base_alpha)\n",
    "    #    plt.figure()#\n",
    "\n",
    "    #    plt.imshow(img_alpha)\n",
    "    #    plt.show()\n",
    "    #    out = base_alpha[:,:,:3]\n",
    "    #if you don't have enblend, you can use one of these\n",
    "    #merged_img = simple_merge(base_warped, img_warped, base_mask, img_mask)\n",
    "    #merged_img = minimum_cost_merge(base_warped, img_warped, base_mask, img_mask)\n",
    "    #merged_edges = remove_empty_edges(merged_img)\n",
    "    return tmp_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imsave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#patches = patchmaker(img_col[0])\n",
    "if 0:\n",
    "    unmatched = img_col\n",
    "    params = [[1, 500, 50], [2, 800, 20], [1000, 1000, 10], [1000, 10000, 7]]\n",
    "    for param in params:\n",
    "        matched = find_all_matches(unmatched, [], param[0], param[1], param[2], False)\n",
    "        print('found %s matches with' %len(matched), param)\n",
    "    print(\"HAVE %s MATCHES\" %len(matched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    for xx, timg in enumerate(matched):\n",
    "        plt.figure(figsize=(14,14))\n",
    "        plt.title(\"NUM %s NAME %s\" %(xx, timg['name']))\n",
    "        plt.imshow(timg['img'])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_find_all_matches(i):\n",
    "    \"\"\"Convert list to arguments for find_all_matches to work with multiprocessing\n",
    "    pool\"\"\"\n",
    "    return find_all_matches(*i)\n",
    "\n",
    "#all_matched = find_all_matches(unmatched, [], param[0], param[1], param[2], False)\n",
    "\n",
    "#pool = Pool(processes=cpu_count())\n",
    "#split_unmatched = [img_col[:6]]\n",
    "#params = [[], 1, 500, 50]\n",
    "#all_matched = pool.map(split_find_all_matches, \n",
    "#                       itertools.izip(split_unmatched, \n",
    "#                                      itertools.repeat(params)))\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE\n"
     ]
    }
   ],
   "source": [
    "print(\"HE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itertools.izip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [4, 5, 6]\n",
    "a = {'stuff':2}\n",
    "b = {'333':2}\n",
    "z = [a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, {'stuff': 2}, {'333': 2}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.extend(z)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
